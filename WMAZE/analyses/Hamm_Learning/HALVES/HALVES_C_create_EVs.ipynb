{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model HALVES -- Conditional Trials\n",
    "## Compares periods of learning and nonlearning between early (run1) and late (run2)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Objective: \n",
    "Identify and isolate fixed preceding conditional trials based on learning curve (probability of correct response) for conditionals. Sort into early and late learning (Run1 vs Run2). \n",
    "**Create EV files for four (4) regressors: early_learning, early_nonlearning, late_learning, and late_nonlearning.\n",
    "\n",
    "Strategy: \n",
    "Use the first derivative of the learning curve to operationalize \"learning\" and \"nonlearning\". Then sort each into trials falling into the first and second run. \n",
    "**Trials with a derivative value of <= 0 are \"nonlearning\", while those with a positive derivative value are \"learning\". \n",
    "\n",
    "Important details: \n",
    "1. 80 conditional trials per \"stimulus set\", 40 per \"run\"\n",
    "2. Problematic conditional trials need to be excluded if:\n",
    "    - 1st trial of a run\n",
    "    - Last 3 trials\n",
    "    - Preceded by a baseline trial\n",
    "    - Participant does not respond\n",
    "3. Learning curve and derivative files collapse Run1 and Run2 into the 80 conditional trials per stimulus set (3)\n",
    "4. Functional imaging data is recorded by individual runs (6), 2 runs per stimulus set\n",
    "5. Details 3 and 4 make things a bit more complicated\n",
    "\n",
    "Strategy: \n",
    "1. Loop through subjects and glob necessary files\n",
    "2. Loop through stimulus sets, load behavioral text files for Run1 and Run2 with Pandas. For stimulus onset (\"StimOnset\"), response (\"Resp\"), and trial type (\"TrialType\"), combine Run1 and Run2 into single Numpy array.\n",
    "3. Use np.where() to grab the index of trials matching specific criteria (e.g. conditional, conditional preceded by baseline, nonresponse trials, or \"most remaining\")\n",
    "    - \"most_remaining\" contains trials either (1) not followed by a conditional, or (2) baselines.\n",
    "4. Address Important Detail 2 -- create an array containing the indices of all problematic conditional trials (bad Bs). Loop through all conditional trials. If a trial fits any of the \"bad\" criteria, its index in the behavioral file [0:320] is added to the \"bad_Bs\" array.\n",
    "5. Use list comprehension to iterate through all conditional trials [0:80]. If identified as problematic, its index within B trials is added to \"bad_B_ind\", numerically sorted, and reversed (largest to smallest -- this is for .pop()).\n",
    "6. Any fixed trials preceding problematic conditionals will not be included in regressors on interest, and therefore need to be included in an \"all remaining\" regressor. Take care not to duplicate trials already in \"most_remaining\".\n",
    "7. Create a temp list containing the learning curve values with list() function (80) and remove problematic conditionals. Using \"bad_B_ind\" from Step 5 [0:80], pop all learning curve values at those indices. Save temp list with a more informative name (i.e. \"new_learning\").\n",
    "8. Repeat Step 6 for derivative values. Once saved as \"new_deriv\", split into two new arrays for \"learning\" and non-learning\" using np.where(). Values greater than 0 == learning, values <= 0 == nonlearning. \n",
    "9. Repeat Step 6 with original conditional indices array (\"b_indices\") -- these contain the indices of conditional trials within the larger behavioral file [0:360]. Label new array \"Good_Bs\". \n",
    "10. Use the conditional-specific indices [0:80] obtained in Step 7 for \"learning\" and \"nonlearning\" to then index the behavioral file-specific [0:360], creating two new variables: \"learn_ind\" and \"nonlearn_ind\". These contain the conditional indices within the behavioral file for periods of learning and nonlearning, less the problematic trials.\n",
    "11. Use list comprehension to iterate through all conditional indices in both arrays and sort into Run1 and Run2:  \n",
    "    - if index <= 159 --> run1_learning or run1_nonlearning\n",
    "    - if index > 159 --> run2_learning or run2_nonlearning\n",
    "12. Use these indices in conjunction with the stimulus onset array from Step 2 to obtain the onset time for each fixed trial. Repeat this for all four regressors. \n",
    "13. For level 1 analysis, EV text files require the following data per trial (event): onset time, duration, and amplitude. This information must be formatted into three columns, each row representing a trial. This formatting is achieved as follows:\n",
    "    - Use the np.vstack() function with three arguments\n",
    "        - First argument (onset time): array containing onset times (e.g. \"run1_learn_os\")\n",
    "            - Identifies activation at specifed timepoint\n",
    "        - Second argument (duration): within vstack, create np.ones array, multiply values by 2.5 (float)\n",
    "            - Length of trial (3 seconds)\n",
    "        - Third argument (amplitude): within vstack, create np.ones array (float)\n",
    "            - Predicted amplitude of signal, uniform at 1.0 (unless parametrically modulated)\n",
    "15. Save all four files using np.savetxt(). Use .format() to insert the appropriate set# in the filename.\n",
    "\n",
    "Status:\n",
    "- *Success*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "import os\n",
    "from os.path import join, split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from glob import glob\n",
    "\n",
    "\n",
    "subs = ['WMAZE_001', 'WMAZE_002', 'WMAZE_004', 'WMAZE_005', 'WMAZE_006',\n",
    "        'WMAZE_007', 'WMAZE_008', 'WMAZE_009', 'WMAZE_010', 'WMAZE_012',\n",
    "        'WMAZE_017', 'WMAZE_018', 'WMAZE_019', 'WMAZE_020', 'WMAZE_021',\n",
    "        'WMAZE_022', 'WMAZE_023', 'WMAZE_024', 'WMAZE_026', 'WMAZE_027']\n",
    "sets = ['set1', 'set2', 'set3']\n",
    "\n",
    "\n",
    "##STEP 1\n",
    "#iterate through subjects\n",
    "for sub in subs:\n",
    "    sub_dir = '/home/data/madlab/Mattfeld_WMAZE/sourcedata/behav/{0}'.format(sub) #base directory \n",
    "    save_dir = join(sub_dir, 'model_HALVES_C/')\n",
    "    \n",
    "    #create directory for save data\n",
    "    if not os.path.exists(save_dir): \n",
    "        os.makedirs(save_dir)\n",
    "    \n",
    "    #grab the derivative files created from the *set-based* learning analysis and sort\n",
    "    frst_deriv_files = sorted(glob(join(sub_dir, 'Bprime_pmode_set*.txt')))     \n",
    "    learning_files = sorted(glob(join(sub_dir, 'B_pmode_set*.txt')))     \n",
    "    behav_runs = sorted(glob(join(sub_dir, '{0}_wmazebl_2015*.txt'.format(sub))))\n",
    "    \n",
    "    \n",
    "    ##STEP 2\n",
    "    #iterate through stimulus sets\n",
    "    for i, curr_set in enumerate(sets): #load derivative, learning curve, and behavioral files\n",
    "        deriv_file = np.loadtxt(frst_deriv_files[i])\n",
    "        learning_curve = np.loadtxt(learning_files[i]) \n",
    "        #grab two runs for each stimulus set\n",
    "        behav_run1 = pd.read_table(behav_runs[i*2])\n",
    "        behav_run2 = pd.read_table(behav_runs[i*2+1])\n",
    "                \n",
    "        #info concerning onset time\n",
    "        behav_os = np.empty(320, dtype=object)\n",
    "        behav_os[:160] = behav_run1['StimOnset'].values\n",
    "        behav_os[160:] = behav_run2['StimOnset'].values + (197*2) #number of volumes x TR \n",
    "        #info concerning subject response\n",
    "        behav_resp = np.empty(320, dtype=object)\n",
    "        behav_resp[:160] = behav_run1['Resp'].values\n",
    "        behav_resp[160:] = behav_run2['Resp'].values        \n",
    "        #info concerning trial type\n",
    "        behav_type = np.empty(320, dtype=object)\n",
    "        behav_type[:160] = behav_run1['TrialType'].values\n",
    "        behav_type[160:] = behav_run2['TrialType'].values\n",
    "        \n",
    "        \n",
    "        ##STEP 3\n",
    "        #identify position (index) of conditional trials\n",
    "        b_indices = np.where((behav_type=='B'))[0] #Bs in the original dataset without preceeding BL trials\n",
    "        trial_shift = behav_type[:-1] #remove last trial in behav_type\n",
    "        trial_shift = np.insert(trial_shift,0,-1) #shifted down to identify BL --> B (places \"-1\" at 0 index)\n",
    "        b_BL_indices = np.where((behav_type=='B')&(trial_shift=='BL'))[0] #B trials with preceeding BLs\n",
    "        trial_shift2 = behav_type[1:] \n",
    "        trial_shift2 = np.append(trial_shift2,-1) #shifted up to grab anything after B (places \"-1\" at last index)\n",
    "        #identify conditional, baseline and nonresponse trials\n",
    "        most_remaining = np.where((behav_type =='B')|(behav_type=='BL')|((trial_shift2!='B')\n",
    "                                  &((behav_type=='A')|(behav_type=='C'))))[0]\n",
    "\n",
    "        \n",
    "        ##STEP 4\n",
    "        #isolate bad Bs - value represents index among all trials in set [0:360] \n",
    "        bad_Bs = [] \n",
    "        bad_Bs.extend(b_BL_indices)\n",
    "        for curr_B in b_indices:\n",
    "            if not curr_B in bad_Bs:\n",
    "                if behav_resp[curr_B] == 'NR': \n",
    "                    bad_Bs.append(curr_B)    \n",
    "                if curr_B in [0, 157, 158, 159, 160, 317, 318, 319]:  \n",
    "                    bad_Bs.append(curr_B)\n",
    "        bad_Bs.sort() \n",
    "             \n",
    "            \n",
    "        ##STEP 5         \n",
    "        #obtains B-specific array indices [0:80] to match length of curve and derivative files \n",
    "        bad_B_ind = sorted([j for j, f in enumerate(b_indices) if f in bad_Bs]) #get bad Bs within Bs \n",
    "        bad_B_ind = bad_B_ind[::-1] #reverse order of Bs to be removed (required for .pop()) \n",
    "        \n",
    "        \n",
    "        ##Step 6\n",
    "        #add fixed before bad Bs to create complete all_remaining\n",
    "        nonBL_bad_Bs = [f for f in bad_Bs if f not in b_BL_indices] #check for bad conditionals not preceded by BLs\n",
    "        fixed_bads = np.array([x - 1 for x in nonBL_bad_Bs]) #subtract one from each index, convert into numpy array\n",
    "        all_remaining = np.concatenate((most_remaining, fixed_bads)) #merge fixed_bads and most_remaining\n",
    "        all_remaining = [int(f) for f in all_remaining] #make sure all arrays are integers (WMAZE_007 is not)\n",
    "      \n",
    "        \n",
    "        ##STEP 7\n",
    "        #remove bad conditional trials from learning curve\n",
    "        temp = list(learning_curve) #temp version of learning_curve\n",
    "        for curr_bad_B in bad_B_ind: \n",
    "            temp.pop(curr_bad_B) #pop out the bad Bs starting from the end   \n",
    "        new_learning = np.array(temp) #save without the removed Bs\n",
    "     \n",
    "    \n",
    "        ##STEP 8\n",
    "        #remove bad conditional trials from derivative\n",
    "        temp1 = list(deriv_file)\n",
    "        for curr_bad_B in bad_B_ind:\n",
    "            temp1.pop(curr_bad_B)  \n",
    "        new_deriv = np.array(temp1[1:]) #remove 1st values in derivative file (0)      \n",
    "        learning = np.where(new_deriv > 0)[0] #values greater than 0 == \"learning\"\n",
    "        nonlearning = np.where(new_deriv <= 0)[0] #values <= 0 == \"nonlearning\"\n",
    "\n",
    "        \n",
    "        ##STEP 9\n",
    "        #use B-specific indices [0:80] to remove bad conditionals from original array of B indices [0:360]\n",
    "        temp2 = list(b_indices)\n",
    "        for curr_bad_B in bad_B_ind:\n",
    "            temp2.pop(curr_bad_B)\n",
    "        good_Bs = np.array(temp2)\n",
    "                \n",
    "        \n",
    "        ##STEP 10\n",
    "        #index good conditional indices using the learning/nonlearning indices\n",
    "        learn_ind = good_Bs[learning]\n",
    "        nonlearn_ind = good_Bs[nonlearning]\n",
    "    \n",
    "        \n",
    "        ##STEP 11\n",
    "        #use list comprehension to grab values matching index criteria for Run1 and Run2\n",
    "        run1_learn = [f for f in learn_ind if f <= 156]\n",
    "        run1_nonlearn = [f for f in nonlearn_ind if f <= 156]       \n",
    "        run2_learn = [f for f in learn_ind if f > 160]\n",
    "        run2_nonlearn = [f for f in nonlearn_ind if f > 160]\n",
    "        \n",
    "        \n",
    "        ##Step 12\n",
    "\n",
    "        #index behavioral onset array using fixed trial indices\n",
    "        run1_learn_os = behav_os[run1_learn]\n",
    "        run1_nonlearn_os = behav_os[run1_nonlearn]\n",
    "        run2_learn_os = behav_os[run2_learn]\n",
    "        run2_nonlearn_os = behav_os[run2_nonlearn]\n",
    "        all_remaining_os = behav_os[all_remaining]\n",
    "        \n",
    "        \n",
    "        ##STEP 13\n",
    "        #create matrices in the format required by first level Bunch()\n",
    "        early_learn_mtrx = np.vstack((run1_learn_os,np.ones(len(run1_learn_os))*2.5, \n",
    "                                     np.ones(len(run1_learn_os)))).T                \n",
    "        early_nonlearn_mtrx = np.vstack((run1_nonlearn_os,np.ones(len(run1_nonlearn_os))*2.5,\n",
    "                                        np.ones(len(run1_nonlearn_os)))).T        \n",
    "        late_learn_mtrx = np.vstack((run2_learn_os,np.ones(len(run2_learn_os))*2.5, \n",
    "                                     np.ones(len(run2_learn_os)))).T       \n",
    "        late_nonlearn_mtrx = np.vstack((run2_nonlearn_os,np.ones(len(run2_nonlearn_os))*2.5,\n",
    "                                        np.ones(len(run2_nonlearn_os)))).T       \n",
    "        all_remaining_mtrx = np.vstack((all_remaining_os,np.ones(len(all_remaining_os))*2.5, \n",
    "                                        np.ones(len(all_remaining_os)))).T\n",
    "        \n",
    "        if not os.path.exists(save_dir):\n",
    "            os.makedirs(save_dir)        \n",
    "        #write EVs to .txt file\n",
    "        for trial in ['early_learn', 'early_nonlearn', 'late_learn', 'late_nonlearn', 'all_remaining']: \n",
    "            exec('np.savetxt(save_dir + \"{0}_{1}.txt\", {1}_mtrx, delimiter=\"\\t\", fmt=\"%.4f\")'.format(curr_set,trial))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
