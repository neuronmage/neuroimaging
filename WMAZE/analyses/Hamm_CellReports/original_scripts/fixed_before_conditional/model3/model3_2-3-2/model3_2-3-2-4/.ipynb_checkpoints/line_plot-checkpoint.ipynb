{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "sids = ['WMAZE_001', 'WMAZE_002', 'WMAZE_004', 'WMAZE_005', 'WMAZE_006',\n",
    "        'WMAZE_007', 'WMAZE_008', 'WMAZE_009', 'WMAZE_010', 'WMAZE_012',\n",
    "        'WMAZE_017', 'WMAZE_018', 'WMAZE_019', 'WMAZE_020', 'WMAZE_021',  \n",
    "        'WMAZE_022', 'WMAZE_023', 'WMAZE_024', 'WMAZE_026', 'WMAZE_027']\n",
    "\n",
    "#sids = ['WMAZE_021']\n",
    "\n",
    "proj_dir = '/home/data/madlab/data/mri/wmaze'\n",
    " \n",
    "mask_filenames = []\n",
    "cope_files = []\n",
    "\n",
    "for SID in sids:\n",
    "    # Grab the mask files for each subject\n",
    "    mask_filenames_glob = glob(proj_dir + '/roi_analysis/mask/anat_masks/_subject_id_' + SID + '/_anatmask_xfm*/*')\n",
    "    mask_filenames.append(sorted(mask_filenames_glob))\n",
    "    # Grab the cope files for each subject\n",
    "    subjcopes_glob = glob(proj_dir + '/frstlvl/wmaze_MRthesis/fixed_before_conditional/model3_2-3-2-5/merge_copes/'\n",
    "                          + SID + '/merged/cope_*')\n",
    "    cope_files.append(sorted(subjcopes_glob))\n",
    "    # If there are no cope files, print subject id\n",
    "    if len(cope_files[-1]) == 0:\n",
    "        print(SID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nibabel as nb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "#sns.set_style(\"ticks\")\n",
    "sns.set_palette('muted')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "#Dictionary containing multiple arrays referenced by keywords\n",
    "all_data = {'subjid':[],\n",
    "            'lh_hp_nonlearning':[], 'rh_hp_nonlearning':[], \n",
    "            'lh_hp_learning':[], 'rh_hp_learning':[],                 \n",
    "            'lh_caudate_nonlearning':[], 'rh_caudate_nonlearning':[], \n",
    "            'lh_caudate_learning':[], 'rh_caudate_learning':[],  \n",
    "            'lh_dlPFC_nonlearning':[], 'rh_dlPFC_nonlearning':[], \n",
    "            'lh_dlPFC_learning':[], 'rh_dlPFC_learning':[],            \n",
    "            'lh_mPFC_nonlearning':[], 'rh_mPFC_nonlearning':[], \n",
    "            'lh_mPFC_learning':[], 'rh_mPFC_learning':[]}\n",
    "\n",
    "corr_matrix = np.zeros((len(sids), 5))\n",
    "\n",
    "# For each interation in the length of subject ids\n",
    "for i in range(len(sids)):\n",
    "    #Append the subject id\n",
    "    all_data['subjid'].append(sids[i])\n",
    "    #Load the left and right hippocampus mask files using Nibabel \n",
    "    lh_hp_img = nb.load(mask_filenames[i][1])\n",
    "    rh_hp_img = nb.load(mask_filenames[i][7])\n",
    "    \n",
    "    lh_caudate_img = nb.load(mask_filenames[i][5])\n",
    "    rh_caudate_img = nb.load(mask_filenames[i][10])\n",
    "    \n",
    "    lh_dlPFC_img = nb.load(mask_filenames[i][0])\n",
    "    rh_dlPFC_img = nb.load(mask_filenames[i][6])\n",
    "    \n",
    "    lh_mPFC_img = nb.load(mask_filenames[i][12])\n",
    "    rh_mPFC_img = nb.load(mask_filenames[i][13])\n",
    "    \n",
    "    \n",
    "    #Load copes for learning vs nonlearning betas\n",
    "    cope_learning_img = nb.load(cope_files[i][0])\n",
    "    cope_nonlearning_img = nb.load(cope_files[i][1])\n",
    "     \n",
    "  \n",
    "    ########################\n",
    "    ### Left Hippocampus ###\n",
    "    ########################\n",
    "    \n",
    "    # cope_nonlearning\n",
    "    lh_hp_nonlearning_data = cope_nonlearning_img.get_data()[lh_hp_img.get_data() > 0.]\n",
    "    all_data['lh_hp_nonlearning'].append(lh_hp_nonlearning_data[0:-1])\n",
    "    \n",
    "    # cope_learning\n",
    "    lh_hp_learning_data = cope_learning_img.get_data()[lh_hp_img.get_data() > 0.]\n",
    "    all_data['lh_hp_learning'].append(lh_hp_learning_data[0:-1])\n",
    "    \n",
    "    #########################\n",
    "    ### Right Hippocampus ###\n",
    "    #########################\n",
    "    \n",
    "    # cope_nonlearning\n",
    "    rh_hp_nonlearning_data = cope_nonlearning_img.get_data()[rh_hp_img.get_data() > 0.]\n",
    "    all_data['rh_hp_nonlearning'].append(rh_hp_nonlearning_data[0:-1])\n",
    "\n",
    "    # cope_learning\n",
    "    rh_hp_learning_data = cope_learning_img.get_data()[rh_hp_img.get_data() > 0.]\n",
    "    all_data['rh_hp_learning'].append(rh_hp_learning_data[0:-1])\n",
    "    \n",
    "    #####################\n",
    "    ### Left Caudate ###\n",
    "    ##################### \n",
    "    \n",
    "    # cope_nonlearning\n",
    "    lh_caudate_nonlearning_data = cope_nonlearning_img.get_data()[lh_caudate_img.get_data() > 0.]\n",
    "    all_data['lh_caudate_nonlearning'].append(lh_caudate_nonlearning_data[0:-1])\n",
    "\n",
    "    # cope_learning\n",
    "    lh_caudate_learning_data = cope_learning_img.get_data()[lh_caudate_img.get_data() > 0.]\n",
    "    all_data['lh_caudate_learning'].append(lh_caudate_learning_data[0:-1])\n",
    "    \n",
    "    #####################\n",
    "    ### Right Caudate ###\n",
    "    ##################### \n",
    "    \n",
    "    # cope_nonlearning\n",
    "    rh_caudate_nonlearning_data = cope_nonlearning_img.get_data()[rh_caudate_img.get_data() > 0.]\n",
    "    all_data['rh_caudate_nonlearning'].append(rh_caudate_nonlearning_data[0:-1])\n",
    "\n",
    "    # cope_learning\n",
    "    rh_caudate_learning_data = cope_learning_img.get_data()[rh_caudate_img.get_data() > 0.]\n",
    "    all_data['rh_caudate_learning'].append(rh_caudate_learning_data[0:-1])  \n",
    "    \n",
    "    ####################\n",
    "    #### Left mPFC #####\n",
    "    ####################\n",
    "    \n",
    "    # cope_nonlearning\n",
    "    lh_mPFC_nonlearning_data = cope_nonlearning_img.get_data()[lh_mPFC_img.get_data() > 0.]\n",
    "    all_data['lh_mPFC_nonlearning'].append(lh_mPFC_nonlearning_data[0:-1])\n",
    "    \n",
    "    # cope_learning\n",
    "    lh_mPFC_learning_data = cope_learning_img.get_data()[lh_mPFC_img.get_data() > 0.]\n",
    "    all_data['lh_mPFC_learning'].append(lh_mPFC_learning_data[0:-1])    \n",
    "    \n",
    "    #####################\n",
    "    #### Right mPFC #####\n",
    "    #####################\n",
    "    \n",
    "    # cope_nonlearning\n",
    "    rh_mPFC_nonlearning_data = cope_nonlearning_img.get_data()[rh_mPFC_img.get_data() > 0.]\n",
    "    all_data['rh_mPFC_nonlearning'].append(rh_mPFC_nonlearning_data[0:-1])\n",
    "\n",
    "    # cope_learning\n",
    "    rh_mPFC_learning_data = cope_learning_img.get_data()[rh_mPFC_img.get_data() > 0.]\n",
    "    all_data['rh_mPFC_learning'].append(rh_mPFC_learning_data[0:-1]) \n",
    "    \n",
    "    ####################\n",
    "    #### Left dlPFC ####\n",
    "    ####################\n",
    "    \n",
    "    # cope_nonlearning\n",
    "    lh_dlPFC_nonlearning_data = cope_nonlearning_img.get_data()[lh_dlPFC_img.get_data() > 0.]\n",
    "    all_data['lh_dlPFC_nonlearning'].append(lh_dlPFC_nonlearning_data[0:-1])\n",
    "    \n",
    "    # cope_learning\n",
    "    lh_dlPFC_learning_data = cope_learning_img.get_data()[lh_dlPFC_img.get_data() > 0.]\n",
    "    all_data['lh_dlPFC_learning'].append(lh_dlPFC_learning_data[0:-1])  \n",
    "    \n",
    "    #####################\n",
    "    ### Right dlPFC ###\n",
    "    #####################\n",
    "    \n",
    "    # cope_nonlearning\n",
    "    rh_dlPFC_nonlearning_data = cope_nonlearning_img.get_data()[rh_dlPFC_img.get_data() > 0.]\n",
    "    all_data['rh_dlPFC_nonlearning'].append(rh_dlPFC_nonlearning_data[0:-1])\n",
    "\n",
    "    # cope_learning\n",
    "    rh_dlPFC_learning_data = cope_learning_img.get_data()[rh_dlPFC_img.get_data() > 0.]\n",
    "    all_data['rh_dlPFC_learning'].append(rh_dlPFC_learning_data[0:-1])    \n",
    "    \n",
    "    #array with keys for each part of all_data dictionary\n",
    "    all_keys = ['lh_hp_nonlearning', 'rh_hp_nonlearning', \n",
    "                'lh_hp_learning', 'rh_hp_learning',                 \n",
    "                'lh_caudate_nonlearning', 'rh_caudate_nonlearning', \n",
    "                'lh_caudate_learning', 'rh_caudate_learning',  \n",
    "                'lh_dlPFC_nonlearning', 'rh_dlPFC_nonlearning', \n",
    "                'lh_dlPFC_learning', 'rh_dlPFC_learning',            \n",
    "                'lh_mPFC_nonlearning', 'rh_mPFC_nonlearning', \n",
    "                'lh_mPFC_learning', 'rh_mPFC_learning']\n",
    "    \n",
    "    for key in all_keys: \n",
    "        # averaging each column for only the current participant \n",
    "        all_data[key][-1] = np.mean(all_data[key][-1], axis = 0)\n",
    "       \n",
    "    #Combined Hippocampus average for nonlearning and learning for current sub\n",
    "    all_data['hp_nonlearning'] = (np.array(all_data['lh_hp_nonlearning'][-1]) + \n",
    "                              np.array(all_data['rh_hp_nonlearning'][-1]))/2.\n",
    "    all_data['hp_learning'] = (np.array(all_data['lh_hp_learning'][-1]) + \n",
    "                               np.array(all_data['rh_hp_learning'][-1]))/2.\n",
    "                             \n",
    "    #Combined Caudate average for nonlearning and learning for current sub\n",
    "    all_data['caudate_nonlearning'] = (np.array(all_data['lh_caudate_nonlearning'][-1]) + \n",
    "                                   np.array(all_data['rh_caudate_nonlearning'][-1]))/2.\n",
    "    all_data['caudate_learning'] = (np.array(all_data['lh_caudate_learning'][-1]) + \n",
    "                                    np.array(all_data['rh_caudate_learning'][-1]))/2.\n",
    "\n",
    "    # Combined mPFC average for nonlearning and learning for current sub\n",
    "    all_data['mPFC_nonlearning'] = (np.array(all_data['lh_mPFC_nonlearning'][-1]) + \n",
    "                                np.array(all_data['rh_mPFC_nonlearning'][-1]))/2.\n",
    "    all_data['mPFC_learning'] = (np.array(all_data['lh_mPFC_learning'][-1]) + \n",
    "                                 np.array(all_data['rh_mPFC_learning'][-1]))/2.\n",
    "\n",
    "    #Combined dlPFC average for nonlearning and learning for current sub\n",
    "    all_data['dlPFC_nonlearning'] = (np.array(all_data['lh_dlPFC_nonlearning'][-1]) + \n",
    "                                 np.array(all_data['rh_dlPFC_nonlearning'][-1]))/2.\n",
    "    all_data['dlPFC_learning'] = (np.array(all_data['lh_dlPFC_learning'][-1]) + \n",
    "                                  np.array(all_data['rh_dlPFC_learning'][-1]))/2.      \n",
    "\n",
    "    y = all_data['hp_learning'] #blue\n",
    "    ax1 = []\n",
    "    for m in range(0, len(y), 1):\n",
    "        ax1.append(m)\n",
    "    x1 = ax1\n",
    "    \n",
    "    z = all_data['mPFC_learning'] #purple\n",
    "    ax2 = []\n",
    "    for n in range(0, len(z), 1):\n",
    "        ax2.append(n)\n",
    "    x2 = ax2\n",
    "    \n",
    "    a = all_data['hp_nonlearning'] #blue\n",
    "    ax3 = []\n",
    "    for o in range(0, len(a), 1):\n",
    "        ax3.append(o)\n",
    "    x3 = ax3\n",
    "    \n",
    "    b = all_data['mPFC_nonlearning'] #purple\n",
    "    ax4 = []\n",
    "    for p in range(0, len(b), 1):\n",
    "        ax4.append(p)\n",
    "    x4 = ax4\n",
    "\n",
    "    fig, (ax, ax2) = plt.subplots(ncols = 2, figsize=(11, 4), sharex = False, sharey = True)\n",
    "    ax.plot(x1, y, color = '#5c0eb5', alpha = 1)\n",
    "    #ax.scatter(x1, y, c = '#5c0eb5', alpha = 1)\n",
    "    ax.plot(x2, z, color = '#f97502', alpha = 0.75)\n",
    "    #ax.scatter(x2, z, c = '#f97502', alpha = 0.5)\n",
    "    ax.set_ylabel('Activation')\n",
    "    ax.set_xlabel('Trials')\n",
    "    ax.set_title('Learning')\n",
    "    ax.legend(['Hippocampus', 'mPFC'], loc = 1)\n",
    "     \n",
    "    ax2.plot(x3, a, color = '#5c0eb5', alpha = 1)\n",
    "    #ax2.scatter(x3, a, c = '#5c0eb5', alpha = 0.5)\n",
    "    ax2.plot(x4, b, color = '#f97502', alpha = 0.75)\n",
    "    #ax2.scatter(x4, b, c = '#f97502', alpha = 0.5)\n",
    "    ax2.set_xlabel('Trials')\n",
    "    ax2.set_title('Non-Learning')\n",
    "    ax2.legend(['Hippocampus', 'mPFC'], loc = 1)\n",
    "    #plt.savefig(\"/home/data/madlab/scripts/wmaze/anal_MR_thesis/fixed_before_conditional/model3/lineplot.jpg\")\n",
    "    print all_data['subjid'][-1]\n",
    "    print \"Learning\", stats.pearsonr(all_data['hp_learning'], all_data['mPFC_learning'])\n",
    "    print \"Nonlearning\", stats.pearsonr(all_data['hp_nonlearning'], all_data['mPFC_nonlearning'])\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
