{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 3 Version 2.3.2.5\n",
    "## Isolating trials of learning -- positive 2nd derivative value, negative 2nd derivative value\n",
    "### Removing last 3 volumes using FSL ROI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import shutil \n",
    "import os\n",
    "from os.path import join, split, basename\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "from pylab import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning\n",
      "[ 0  1  7  8  9 12 19 20 21 47 48]\n",
      "nonlearning\n",
      "[ 2  3  4  5  6 10 11 13 14 15 16 17 18 22 23 24 25 26 27 28 29 30 31 32 33\n",
      " 34 35 36 37 38 39 40 41 42 43 44 45 46 49 50 51 52 53 54 55 56 57 58]\n",
      "learning\n",
      "[ 0  1  5  6  9 10 15 17 18 19 21 22 29 31 42 43 44 45 50 51 53 54]\n",
      "nonlearning\n",
      "[ 2  3  4  7  8 11 12 13 14 16 20 23 24 25 26 27 28 30 32 33 34 35 36 37 38\n",
      " 39 40 41 46 47 48 49 52 55 56]\n",
      "learning\n",
      "[ 0  1 11 13 14 15 16 20 21 26 27 28 29 30 56 58]\n",
      "nonlearning\n",
      "[ 2  3  4  5  6  7  8  9 10 12 17 18 19 22 23 24 25 31 32 33 34 35 36 37 38\n",
      " 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 57 59 60]\n",
      "learning\n",
      "[ 1  2  9 10 11 13 17 18 20 23 31 32 39 40 41 45 46 50]\n",
      "nonlearning\n",
      "[ 0  3  4  5  6  7  8 12 14 15 16 19 21 22 24 25 26 27 28 29 30 33 34 35 36\n",
      " 37 38 42 43 44 47 48 49 51 52]\n",
      "learning\n",
      "[ 0  1  2  6  7  8 15 21 22 23 24 31 32 35 36 37 38 42 46 49 53 54]\n",
      "nonlearning\n",
      "[ 3  4  5  9 10 11 12 13 14 16 17 18 19 20 25 26 27 28 29 30 33 34 39 40 41\n",
      " 43 44 45 47 48 50 51 52 55 56]\n",
      "learning\n",
      "[ 1  2  3  4  7 13 14 15 20 21 26 27 28 31 32 33 47 48 49 59]\n",
      "nonlearning\n",
      "[ 0  5  6  8  9 10 11 12 16 17 18 19 22 23 24 25 29 30 34 35 36 37 38 39 40\n",
      " 41 42 43 44 45 46 50 51 52 53 54 55 56 57 58]\n",
      "learning\n",
      "[ 3  4  9 10 17 20 23 24 25 29 30 31 32 35 36 38 39 40 41 58 59 60 61 62]\n",
      "nonlearning\n",
      "[ 0  1  2  5  6  7  8 11 12 13 14 15 16 18 19 21 22 26 27 28 33 34 37 42 43\n",
      " 44 45 46 47 48 49 50 51 52 53 54 55 56 57]\n",
      "learning\n",
      "[ 3  4  5  6  7  8 11 31 32 50 51]\n",
      "nonlearning\n",
      "[ 0  1  2  9 10 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 33\n",
      " 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 52 53 54 55 56 57 58 59 60]\n",
      "learning\n",
      "[ 0  1  7  8 15 16 17 18 19 28 29 30 38 39 47 48 49 56 57]\n",
      "nonlearning\n",
      "[ 2  3  4  5  6  9 10 11 12 13 14 20 21 22 23 24 25 26 27 31 32 33 34 35 36\n",
      " 37 40 41 42 43 44 45 46 50 51 52 53 54 55]\n",
      "learning\n",
      "[ 0  1  2  5  6  7  8  9 11 14 15 16 22 23 24 25 26 27 28 34 36 37 42 43 44\n",
      " 45 49 50 51 52 53]\n",
      "nonlearning\n",
      "[ 3  4 10 12 13 17 18 19 20 21 29 30 31 32 33 35 38 39 40 41 46 47 48 54 55]\n",
      "learning\n",
      "[ 0  1  2  4 14 15 16 17 21 22 23 24 25 47 51 52]\n",
      "nonlearning\n",
      "[ 3  5  6  7  8  9 10 11 12 13 18 19 20 26 27 28 29 30 31 32 33 34 35 36 37\n",
      " 38 39 40 41 42 43 44 45 46 48 49 50 53 54 55 56]\n",
      "learning\n",
      "[ 2  6  9 10 20 21 30 31 32 36 37 44 45 46 47 48 49 50 59 60]\n",
      "nonlearning\n",
      "[ 0  1  3  4  5  7  8 11 12 13 14 15 16 17 18 19 22 23 24 25 26 27 28 29 33\n",
      " 34 35 38 39 40 41 42 43 51 52 53 54 55 56 57 58 61]\n",
      "learning\n",
      "[ 0  1  6  7 10 15 16 19 20 21 23 24 25 27 28 34 36 38 42 45 46 47 48]\n",
      "nonlearning\n",
      "[ 2  3  4  5  8  9 11 12 13 14 17 18 22 26 29 30 31 32 33 35 37 39 40 41 43\n",
      " 44 49 50 51]\n",
      "learning\n",
      "[ 1  5 12 13 14 24 25 26 29 30 31 35 41 42 43 44 48 52 53 58]\n",
      "nonlearning\n",
      "[ 0  2  3  4  6  7  8  9 10 11 15 16 17 18 19 20 21 22 23 27 28 32 33 34 36\n",
      " 37 38 39 40 45 46 47 49 50 51 54 55 56 57 59]\n",
      "learning\n",
      "[ 0  1  2  3  6 11 12 13 14 15 19 21 25 28 31 32 33 34 37 38 41 42]\n",
      "nonlearning\n",
      "[ 4  5  7  8  9 10 16 17 18 20 22 23 24 26 27 29 30 35 36 39 40 43 44 45 46\n",
      " 47 48 49 50 51 52 53 54 55]\n",
      "learning\n",
      "[ 0  1  6 13 14 15 16 17 20 21 22 25 28 32 36 39 44 48 49 51]\n",
      "nonlearning\n",
      "[ 2  3  4  5  7  8  9 10 11 12 18 19 23 24 26 27 29 30 31 33 34 35 37 38 40\n",
      " 41 42 43 45 46 47 50 52 53 54]\n",
      "learning\n",
      "[ 2  3  9 10 19 23 24 26 27 32 33 35 40 41 47 51 52 53 54 55]\n",
      "nonlearning\n",
      "[ 0  1  4  5  6  7  8 11 12 13 14 15 16 17 18 20 21 22 25 28 29 30 31 34 36\n",
      " 37 38 39 42 43 44 45 46 48 49 50 56 57 58]\n",
      "learning\n",
      "[ 2  3  6  7  8 11 12 15 17 23 26 27 30 31 40 41 46 47 52 53]\n",
      "nonlearning\n",
      "[ 0  1  4  5  9 10 13 14 16 18 19 20 21 22 24 25 28 29 32 33 34 35 36 37 38\n",
      " 39 42 43 44 45 48 49 50 51 54 55 56]\n",
      "learning\n",
      "[ 0  5  6 10 11 12 16 17 19 22 23 24 30 31 32 33 34 35 36 40 45 46 50 51]\n",
      "nonlearning\n",
      "[ 1  2  3  4  7  8  9 13 14 15 18 20 21 25 26 27 28 29 37 38 39 41 42 43 44\n",
      " 47 48 49 52 53]\n",
      "learning\n",
      "[ 0  5  6  9 10 11 12 13 15 19 20 21 22 25 29 30 31 32 36 37 38 50 53]\n",
      "nonlearning\n",
      "[ 1  2  3  4  7  8 14 16 17 18 23 24 26 27 28 33 34 35 39 40 41 42 43 44 45\n",
      " 46 47 48 49 51 52]\n",
      "learning\n",
      "[ 0  1  3  7 13 14 19 20 21 23 25 26 27 28 29 31 32 33 37 38 52 53 58 59 60\n",
      " 61]\n",
      "nonlearning\n",
      "[ 2  4  5  6  8  9 10 11 12 15 16 17 18 22 24 30 34 35 36 39 40 41 42 43 44\n",
      " 45 46 47 48 49 50 51 54 55 56 57 62]\n",
      "learning\n",
      "[ 1  2  8 10 11 12 13 18 19 20 23 26 27 31 32 33 34 38 39 42 43 49 50]\n",
      "nonlearning\n",
      "[ 0  3  4  5  6  7  9 14 15 16 17 21 22 24 25 28 29 30 35 36 37 40 41 44 45\n",
      " 46 47 48]\n",
      "learning\n",
      "[ 4  5  6 10 11 18 19 20 21 22 25 26 29 30 37 38 43 44 45 46 47 51]\n",
      "nonlearning\n",
      "[ 0  1  2  3  7  8  9 12 13 14 15 16 17 23 24 27 28 31 32 33 34 35 36 39 40\n",
      " 41 42 48 49 50 52]\n",
      "learning\n",
      "[ 3  6  7  8  9 13 14 15 16 19 20 25 26 27 28 34 35 36 37 40 41 42 43 53 55\n",
      " 56]\n",
      "nonlearning\n",
      "[ 0  1  2  4  5 10 11 12 17 18 21 22 23 24 29 30 31 32 33 38 39 44 45 46 47\n",
      " 48 49 50 51 52 54 57 58]\n",
      "learning\n",
      "[ 0  1  3  8 14 16 17 18 22 23 24 27 28 29 30 31 33 34 35 46 51]\n",
      "nonlearning\n",
      "[ 2  4  5  6  7  9 10 11 12 13 15 19 20 21 25 26 32 36 37 38 39 40 41 42 43\n",
      " 44 45 47 48 49 50]\n",
      "learning\n",
      "[ 0  1  2  5  6  8  9 10 17 18 19 20 24 25 32 33 34 35 36 37 42 43 44 48 49\n",
      " 50 51 52 55 56 57]\n",
      "nonlearning\n",
      "[ 3  4  7 11 12 13 14 15 16 21 22 23 26 27 28 29 30 31 38 39 40 41 45 46 47\n",
      " 53 54]\n",
      "learning\n",
      "[ 0  1  2  3  4  8 10 11 12 13 14 18 28 29 30 34 35 36 55]\n",
      "nonlearning\n",
      "[ 5  6  7  9 15 16 17 19 20 21 22 23 24 25 26 27 31 32 33 37 38 39 40 41 42\n",
      " 43 44 45 46 47 48 49 50 51 52 53 54]\n",
      "learning\n",
      "[ 0  3  4  6  7  9 10 14 17 18 19 20 21 25 29 32 33 34 39 42 44 47 50 53]\n",
      "nonlearning\n",
      "[ 1  2  5  8 11 12 13 15 16 22 23 24 26 27 28 30 31 35 36 37 38 40 41 43 45\n",
      " 46 48 49 51 52]\n",
      "learning\n",
      "[ 0  1  6  7 12 15 17 18 28 32 33 35 36 37 39 40 43 48 49 50 51]\n",
      "nonlearning\n",
      "[ 2  3  4  5  8  9 10 11 13 14 16 19 20 21 22 23 24 25 26 27 29 30 31 34 38\n",
      " 41 42 44 45 46 47]\n",
      "learning\n",
      "[ 0  3  4  5 19 20 24 25 26 27 29 30 37 38 39 40 41 44 45 52 53]\n",
      "nonlearning\n",
      "[ 1  2  6  7  8  9 10 11 12 13 14 15 16 17 18 21 22 23 28 31 32 33 34 35 36\n",
      " 42 43 46 47 48 49 50 51 54 55 56 57 58]\n",
      "learning\n",
      "[ 0  1  2  3  4  5  8 12 13 14 18 19 22 24 34 35 36 37 38 39 42 43 44 47 48\n",
      " 49 50 56]\n",
      "nonlearning\n",
      "[ 6  7  9 10 11 15 16 17 20 21 23 25 26 27 28 29 30 31 32 33 40 41 45 46 51\n",
      " 52 53 54 55 57 58 59 60]\n",
      "learning\n",
      "[ 0  4  5  6  7  8  9 10 11 22 23 24 28 33 35 36 41 44 47 48 50]\n",
      "nonlearning\n",
      "[ 1  2  3 12 13 14 15 16 17 18 19 20 21 25 26 27 29 30 31 32 34 37 38 39 40\n",
      " 42 43 45 46 49 51 52 53 54]\n",
      "learning\n",
      "[ 0  1  5 10 11 13 14 15 16 35 36 37 41 47 52 53]\n",
      "nonlearning\n",
      "[ 2  3  4  6  7  8  9 12 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33\n",
      " 34 38 39 40 42 43 44 45 46 48 49 50 51 54 55]\n",
      "learning\n",
      "[ 0  2  3  4  7  8  9 13 14 17 18 19 20 23 24 26 27 28 29 30 31 32 33 45]\n",
      "nonlearning\n",
      "[ 1  5  6 10 11 12 15 16 21 22 25 34 35 36 37 38 39 40 41 42 43 44 46 47 48\n",
      " 49 50 51 52 53]\n",
      "learning\n",
      "[ 0  1  4  8 10 11 12 30 31 37 39 40 44 45 46 47 48]\n",
      "nonlearning\n",
      "[ 2  3  5  6  7  9 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 32 33\n",
      " 34 35 36 38 41 42 43 49 50 51 52 53 54 55 56 57 58 59]\n",
      "learning\n",
      "[ 0  1  6  7  8  9 10 17 18 19 21 23 24 29 30 35 43 44 46 52 53]\n",
      "nonlearning\n",
      "[ 2  3  4  5 11 12 13 14 15 16 20 22 25 26 27 28 31 32 33 34 36 37 38 39 40\n",
      " 41 42 45 47 48 49 50 51 54 55]\n",
      "learning\n",
      "[ 0  2  6  8 12 16 17 18 19 22 45 46 51]\n",
      "nonlearning\n",
      "[ 1  3  4  5  7  9 10 11 13 14 15 20 21 23 24 25 26 27 28 29 30 31 32 33 34\n",
      " 35 36 37 38 39 40 41 42 43 44 47 48 49 50 52 53]\n",
      "learning\n",
      "[ 0  1  4  5  6 28 29 35 36 37 52 53 54 57 58]\n",
      "nonlearning\n",
      "[ 2  3  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 30 31\n",
      " 32 33 34 38 39 40 41 42 43 44 45 46 47 48 49 50 51 55 56 59 60 61 62 63]\n",
      "learning\n",
      "[ 0  1  5  6  7 12 13 26 27 31 38 39 50 51 52 55 56]\n",
      "nonlearning\n",
      "[ 2  3  4  8  9 10 11 14 15 16 17 18 19 20 21 22 23 24 25 28 29 30 32 33 34\n",
      " 35 36 37 40 41 42 43 44 45 46 47 48 49 53 54 57 58 59 60]\n",
      "learning\n",
      "[ 1  2  3  4  5 17 18 19 20 21 23 24 25 26 33 34 37 38 40 45 46 47 49 50 51\n",
      " 52 53 54 55]\n",
      "nonlearning\n",
      "[ 0  6  7  8  9 10 11 12 13 14 15 16 22 27 28 29 30 31 32 35 36 39 41 42 43\n",
      " 44 48 56 57 58 59]\n",
      "learning\n",
      "[ 2  4  6  9 12 13 14 15 20 24 27 31 38 39 40 41 42 43 44 45 53]\n",
      "nonlearning\n",
      "[ 0  1  3  5  7  8 10 11 16 17 18 19 21 22 23 25 26 28 29 30 32 33 34 35 36\n",
      " 37 46 47 48 49 50 51 52 54 55 56 57]\n",
      "learning\n",
      "[ 2  3  6  7 11 12 13 19 20 21 26 27 31 32 36 37 39 40 41 42 43 45 46 47 52\n",
      " 53 54]\n",
      "nonlearning\n",
      "[ 0  1  4  5  8  9 10 14 15 16 17 18 22 23 24 25 28 29 30 33 34 35 38 44 48\n",
      " 49 50 51 55 56]\n",
      "learning\n",
      "[ 2  3  4  5  6  7  9 13 14 15 16 17 18 19 20 21 22 23 24 26 27 33 40 45 46\n",
      " 52 53]\n",
      "nonlearning\n",
      "[ 0  1  8 10 11 12 25 28 29 30 31 32 34 35 36 37 38 39 41 42 43 44 47 48 49\n",
      " 50 51 54]\n",
      "learning\n",
      "[ 0  7  8  9 10 11 12 13 17 18 19 28 29 33 37 38 39 45 48 51 52 53 56 59]\n",
      "nonlearning\n",
      "[ 1  2  3  4  5  6 14 15 16 20 21 22 23 24 25 26 27 30 31 32 34 35 36 40 41\n",
      " 42 43 44 46 47 49 50 54 55 57 58]\n",
      "learning\n",
      "[ 0  9 10 11 12 13 15 16 17 20 21 22 23 24 33 34 35 36 39 42 43 44 45 48 49\n",
      " 52 53 56 57 58 59]\n",
      "nonlearning\n",
      "[ 1  2  3  4  5  6  7  8 14 18 19 25 26 27 28 29 30 31 32 37 38 40 41 46 47\n",
      " 50 51 54 55 60 61 62 63]\n",
      "learning\n",
      "[ 0  1  4 12 14 15 18 33 34 39 43 44 45 46 47 51 52 53]\n",
      "nonlearning\n",
      "[ 2  3  5  6  7  8  9 10 11 13 16 17 19 20 21 22 23 24 25 26 27 28 29 30 31\n",
      " 32 35 36 37 38 40 41 42 48 49 50]\n",
      "learning\n",
      "[ 1  2  3  5 15 16 20 21 22 29 30 33 34 37 43 44 45 48 50 51]\n",
      "nonlearning\n",
      "[ 0  4  6  7  8  9 10 11 12 13 14 17 18 19 23 24 25 26 27 28 31 32 35 36 38\n",
      " 39 40 41 42 46 47 49 52 53 54 55]\n",
      "learning\n",
      "[ 0  1  2  3  4  5  6 14 15 16 28 29 37 38 47 48 60 61 62]\n",
      "nonlearning\n",
      "[ 7  8  9 10 11 12 13 17 18 19 20 21 22 23 24 25 26 27 30 31 32 33 34 35 36\n",
      " 39 40 41 42 43 44 45 46 49 50 51 52 53 54 55 56 57 58 59]\n",
      "learning\n",
      "[ 0  4  7  8  9 11 12 17 20 21 29 31 33 34 38 39 40 41 44 45]\n",
      "nonlearning\n",
      "[ 1  2  3  5  6 10 13 14 15 16 18 19 22 23 24 25 26 27 28 30 32 35 36 37 42\n",
      " 43 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61]\n",
      "learning\n",
      "[ 5  6  7 13 14 27 30 33 37 46]\n",
      "nonlearning\n",
      "[ 0  1  2  3  4  8  9 10 11 12 15 16 17 18 19 20 21 22 23 24 25 26 28 29 31\n",
      " 32 34 35 36 38 39 40 41 42 43 44 45 47 48 49 50 51 52 53 54 55 56 57 58]\n",
      "learning\n",
      "[ 0  7 11 12 13 15 20 26 27 28 29 30 40 41 42 43 47 48 55]\n",
      "nonlearning\n",
      "[ 1  2  3  4  5  6  8  9 10 14 16 17 18 19 21 22 23 24 25 31 32 33 34 35 36\n",
      " 37 38 39 44 45 46 49 50 51 52 53 54]\n",
      "learning\n",
      "[ 2  3  4  5  6  9 10 11 14 15 16 25 26 27 29 34 35 42 43 46 52 56 58]\n",
      "nonlearning\n",
      "[ 0  1  7  8 12 13 17 18 19 20 21 22 23 24 28 30 31 32 33 36 37 38 39 40 41\n",
      " 44 45 47 48 49 50 51 53 54 55 57]\n",
      "learning\n",
      "[ 1  2  3  6  7  9 16 17 20 21 22 23 24 25 27 28 29 36 37 38 39 53]\n",
      "nonlearning\n",
      "[ 0  4  5  8 10 11 12 13 14 15 18 19 26 30 31 32 33 34 35 40 41 42 43 44 45\n",
      " 46 47 48 49 50 51 52 54]\n",
      "learning\n",
      "[ 1  2  9 10 14 15 18 20 30 31 32 33 34 35 38 39 53 54 55 59 60 61 62]\n",
      "nonlearning\n",
      "[ 0  3  4  5  6  7  8 11 12 13 16 17 19 21 22 23 24 25 26 27 28 29 36 37 40\n",
      " 41 42 43 44 45 46 47 48 49 50 51 52 56 57 58 63 64]\n",
      "learning\n",
      "[ 1  2  9 10 11 15 16 17 22 23 30 39 41 42 47 48]\n",
      "nonlearning\n",
      "[ 0  3  4  5  6  7  8 12 13 14 18 19 20 21 24 25 26 27 28 29 31 32 33 34 35\n",
      " 36 37 38 40 43 44 45 46]\n",
      "learning\n",
      "[ 0  1  2  4  5 22 30 31 32 33 34 41 42 47 48 49 50 55 56 57]\n",
      "nonlearning\n",
      "[ 3  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 23 24 25 26 27 28 29 35\n",
      " 36 37 38 39 40 43 44 45 46 51 52 53 54 58 59 60 61]\n",
      "learning\n",
      "[ 0  1  2  3  7 12 13 17 19 20 21 25 28 31 32 38 39 40 44 45 54 55 56 57 58\n",
      " 59 60 61]\n",
      "nonlearning\n",
      "[ 4  5  6  8  9 10 11 14 15 16 18 22 23 24 26 27 29 30 33 34 35 36 37 41 42\n",
      " 43 46 47 48 49 50 51 52 53 62 63 64]\n"
     ]
    }
   ],
   "source": [
    "#cope01_FX_before_COND_incorr_run1_trl1_onset5.72.nii.gz\n",
    "def onset_sort(x):\n",
    "    x = x.split('_')[-1]\n",
    "    return(x[5:-8])\n",
    "\n",
    "#subs = ['WMAZE_001']\n",
    "\n",
    "subs = ['WMAZE_002', 'WMAZE_004', 'WMAZE_005', 'WMAZE_006',\n",
    "        'WMAZE_007', 'WMAZE_008', 'WMAZE_009', 'WMAZE_010', 'WMAZE_012',\n",
    "        'WMAZE_017', 'WMAZE_018', 'WMAZE_019', 'WMAZE_020', 'WMAZE_021',\n",
    "        'WMAZE_022', 'WMAZE_023', 'WMAZE_024', 'WMAZE_026', 'WMAZE_027']\n",
    "\n",
    "sets = ['set1', 'set2', 'set3']\n",
    "runs = ['run1', 'run2', 'run3', 'run4', 'run5', 'run6']\n",
    "\n",
    "\n",
    "for sub in subs:\n",
    "    sub_dir = '/home/data/madlab/data/mri/wmaze/'\n",
    "    \n",
    "    #grab the derivative files created from the set-based learning analysis\n",
    "    frst_deriv_files = glob(join(sub_dir, \n",
    "                                 'scanner_behav/{0}/Bprime_pmode_set*.txt'.format(sub))) \n",
    "    frst_deriv_files.sort() \n",
    "    \n",
    "    #grab the learning curve file for B trials\n",
    "    learning_files = glob(join(sub_dir, 'scanner_behav/{0}/B_pmode_set*.txt'.format(sub))) \n",
    "    learning_files.sort()\n",
    "    \n",
    "    #grab the 95% confidence file for B trials\n",
    "    upper_95_files = glob(join(sub_dir, \n",
    "                               'scanner_behav/{0}/B_p95_set*.txt'.format(sub)))\n",
    "    upper_95_files.sort()\n",
    "    \n",
    "    #grab all cope files created by LSS Model 5\n",
    "    cope_files = glob(join(sub_dir, \n",
    "                          'frstlvl/wmaze_MRthesis/fixed_before_conditional/model3_2-3-2/{0}/'.format(sub),\n",
    "                          'modelfit/contrasts/_estimate_model*/cope*_FX_before_COND_*corr_run*_trl*.nii.gz'))     \n",
    "    cope_files.sort()\n",
    "    #print cope_files\n",
    "    \n",
    "    #grab the behavioral files for all runs\n",
    "    behav_runs = glob(join(sub_dir, \n",
    "                           'scanner_behav/{0}/{0}_wmazebl_2015*.txt'.format(sub))) \n",
    "    behav_runs.sort()\n",
    "    os.makedirs(join('/home/data/madlab/data/mri/wmaze/frstlvl/wmaze_MRthesis/',\n",
    "                     'fixed_before_conditional/model3_2-3-2-5-scndderiv/{0}/learning/'.format(sub)))\n",
    "    \n",
    "    os.makedirs(join('/home/data/madlab/data/mri/wmaze/frstlvl/wmaze_MRthesis/',\n",
    "                     'fixed_before_conditional/model3_2-3-2-5-scndderiv/{0}/nonlearning/'.format(sub)))\n",
    "    \n",
    "    #### LOADING AND ORGANIZING THE COPE FILES ####\n",
    "    all_runs = []\n",
    "    for curr_run in runs:\n",
    "        #selects only the cope files containing the current run's number\n",
    "        curr_run_files = np.array([f for f in cope_files if curr_run in f])\n",
    "        #gets the onset time out of the file names using function\n",
    "        onset_nums = [float(onset_sort(f)) for f in curr_run_files]\n",
    "        sorted_nums = np.argsort(onset_nums)\n",
    "        #arranges the actual files according to onset time\n",
    "        curr_run_files = curr_run_files[sorted_nums]\n",
    "        all_runs.append(curr_run_files)\n",
    "        \n",
    "    \n",
    "    for i, curr_set in enumerate(sets):\n",
    "        #load derivative, learning, and p95 files\n",
    "        frst_deriv = np.loadtxt(frst_deriv_files[i])\n",
    "        #get the second derivative of the learning curve\n",
    "        deriv_file = np.gradient(frst_deriv)\n",
    "        learning_curve = np.loadtxt(learning_files[i]) \n",
    "        upper_95 = np.loadtxt(upper_95_files[i])\n",
    "        \n",
    "        #### COPE FILES ####\n",
    "        #merge the two runs into one array for the current stim set\n",
    "        curr_set_copes = np.concatenate((all_runs[i*2], all_runs[i*2+1]))\n",
    "        #print len(curr_set_copes) \n",
    "        \n",
    "        \n",
    "        #### GETTING THE DERIV FILES TO MATCH NUMBER OF COPES ####\n",
    "        #load behavioral files\n",
    "        behav_run1 = pd.read_table(behav_runs[i*2])\n",
    "        behav_run2 = pd.read_table(behav_runs[i*2+1])\n",
    "        \n",
    "        \n",
    "        #info concerning onset time\n",
    "        behav_os = np.empty(320, dtype=object)\n",
    "        behav_os[:160] = behav_run1['StimOnset'].values\n",
    "        behav_os[160:] = behav_run2['StimOnset'].values\n",
    "        \n",
    "        #info concerning subject response\n",
    "        behav_resp = np.empty(320, dtype=object)\n",
    "        behav_resp[:160] = behav_run1['Resp'].values\n",
    "        behav_resp[160:] = behav_run2['Resp'].values\n",
    "        \n",
    "        #info concerning trial type\n",
    "        behav_type = np.empty(320, dtype=object)\n",
    "        behav_type[:160] = behav_run1['TrialType'].values\n",
    "        behav_type[160:] = behav_run2['TrialType'].values\n",
    "        \n",
    "        trial_shift = behav_type[:-1] \n",
    "        trial_shift = np.insert(trial_shift, 0, -1)\n",
    "        \n",
    "        #indices of all Bs in the original dataset without preceeding BL trials\n",
    "        b_indices = np.where((behav_type == 'B'))[0]\n",
    "        #grabs B trials with preceeding BLs\n",
    "        b_BL_indices = np.where((behav_type == 'B') & (trial_shift == 'BL'))[0]\n",
    "       \n",
    "        \n",
    "        #isolate bad Bs for removal in learning curve/derivative/p95 files\n",
    "        bad_Bs = [] \n",
    "        bad_Bs.extend(b_BL_indices)\n",
    "        for curr_B in b_indices:\n",
    "            if not curr_B in bad_Bs:\n",
    "                #identify in B trials which are non-response\n",
    "                if behav_resp[curr_B] == 'NR': \n",
    "                    bad_Bs.append(curr_B)\n",
    "                #indices if B trial comes first (observed on 001 run 6)    \n",
    "                if curr_B in [0, 157, 158, 159, 160, 317, 318, 319]: \n",
    "                    bad_Bs.append(curr_B)\n",
    "        bad_Bs.sort() \n",
    "        #print len(bad_Bs)        \n",
    "        \n",
    "        #get the indices for the bad Bs within the group of Bs\n",
    "        bad_B_ind = [j for j, f in enumerate(b_indices) if f in bad_Bs] \n",
    "        bad_B_ind.sort()\n",
    "        #reverse order of Bs to be removed\n",
    "        bad_B_ind = bad_B_ind[::-1]\n",
    "        #print len(bad_B_ind)\n",
    "                      \n",
    "        #LEARNING CURVE FILES\n",
    "        #create a temp version of learning_curve\n",
    "        temp2 = list(learning_curve)\n",
    "        #pop out the bad Bs starting from the end\n",
    "        for curr_bad_B in bad_B_ind:\n",
    "            temp2.pop(curr_bad_B)\n",
    "        #save without the removed Bs    \n",
    "        new_learning = np.array(temp2)\n",
    "        #print len(new_learning)\n",
    "        \n",
    "        #P95 FILES\n",
    "        temp3 = list(upper_95)\n",
    "        for curr_bad_B in bad_B_ind:\n",
    "            temp3.pop(curr_bad_B)  \n",
    "        # new upper 95% without bad Bs    \n",
    "        new_upper_95 = np.array(temp3)\n",
    "        #print len(new_upper_95)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #DERIV FILES\n",
    "        temp = list(deriv_file)\n",
    "        for curr_bad_B in bad_B_ind:\n",
    "            temp.pop(curr_bad_B)   \n",
    "        new_deriv = np.array(temp[:-1])\n",
    "        #print new_deriv\n",
    "        #print len(deriv_file)\n",
    "        learning = np.where(new_deriv > 0)[0]\n",
    "        nonlearning = np.where(new_deriv <= 0)[0]\n",
    "        print \"learning\"\n",
    "        print learning\n",
    "        print \"nonlearning\"\n",
    "        print nonlearning\n",
    "         \n",
    "        #remove the bad Bs from the B-list\n",
    "        temp4 = list(b_indices)\n",
    "        for curr_bad_B in bad_B_ind:\n",
    "            temp4.pop(curr_bad_B)\n",
    "        good_Bs = np.array(temp4)\n",
    "        #print len(b_indices)\n",
    "        #print len(good_Bs)\n",
    "        #print good_Bs\n",
    "\n",
    "        #convert original behavioral file indices to new B-specific index\n",
    "        new_indices_B = []\n",
    "        for n, curr_new_B in enumerate(good_Bs):\n",
    "            new_indices_B.append(n)\n",
    "        new_indices_B = np.array(new_indices_B)\n",
    "        #print \"New Bs\"\n",
    "        #print new_indices_B        \n",
    "        #print new_indices_B\n",
    "        #grabs the B trials with a positive value derivative\n",
    "    \n",
    "        b_learning = new_indices_B[learning]\n",
    "        #print b_learning\n",
    "        #grabs the B trials with a zero or negative value derivative\n",
    "        b_nonlearning = new_indices_B[nonlearning]\n",
    "        #print b_nonlearning\n",
    "        fixed_learning_files = curr_set_copes[b_learning]\n",
    "        fixed_nonlearning_files = curr_set_copes[b_nonlearning] \n",
    "        \n",
    "        \n",
    "        #copy and save selected learning files to new folder for merge script\n",
    "        for curr_learning in fixed_learning_files:\n",
    "            learning_basename =  os.path.basename(curr_learning)  \n",
    "            shutil.copy2(curr_learning,\n",
    "                         join('/home/data/madlab/data/mri/wmaze/frstlvl/wmaze_MRthesis/',\n",
    "                              'fixed_before_conditional/model3_2-3-2-5-scndderiv/{0}/'.format(sub),\n",
    "                              'learning/{0}'.format(learning_basename)))\n",
    "\n",
    "        #copy and save selected after files to new folder for merge script   \n",
    "        for curr_nonlearning in fixed_nonlearning_files:\n",
    "            nonlearning_basename =  os.path.basename(curr_nonlearning)\n",
    "            shutil.copy2(curr_nonlearning,\n",
    "                         join('/home/data/madlab/data/mri/wmaze/frstlvl/wmaze_MRthesis/',\n",
    "                              'fixed_before_conditional/model3_2-3-2-5-scndderiv/{0}/'.format(sub),\n",
    "                              'nonlearning/{0}'.format(nonlearning_basename)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
