{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Learn1\n",
    "## Learning --> 4 bins based on learning curve\n",
    "## Deriv --> 4 bins based on derivative of learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import shutil \n",
    "import os\n",
    "from os.path import join, split, basename\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from pylab import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subs = ['WMAZE_001']\n",
    "'''\n",
    "subs = ['WMAZE_001', 'WMAZE_002', 'WMAZE_004', 'WMAZE_005', 'WMAZE_006',\n",
    "        'WMAZE_007', 'WMAZE_008', 'WMAZE_009', 'WMAZE_010', 'WMAZE_012',\n",
    "        'WMAZE_017', 'WMAZE_018', 'WMAZE_019', 'WMAZE_020', 'WMAZE_021',\n",
    "        'WMAZE_022', 'WMAZE_023', 'WMAZE_024', 'WMAZE_026', 'WMAZE_027']\n",
    "'''\n",
    "sets = ['set1', 'set2', 'set3']\n",
    "\n",
    "for sub in subs:\n",
    "    sub_dir = '/home/data/madlab/data/mri/wmaze/'\n",
    "    save_dir = '/home/data/madlab/data/mri/wmaze/scanner_behav/{0}/'.format(sub)\n",
    "    \n",
    "    #grab the derivative files created from the set-based learning analysis\n",
    "    frst_deriv_files = glob(join(sub_dir, 'scanner_behav/{0}/Bprime_pmode_set*.txt'.format(sub))) \n",
    "    frst_deriv_files.sort() \n",
    "    \n",
    "    #grab the learning curve file for B trials\n",
    "    learning_files = glob(join(sub_dir, 'scanner_behav/{0}/B_pmode_set*.txt'.format(sub))) \n",
    "    learning_files.sort()\n",
    "    \n",
    "    #grab the derivative files created from the set-based learning analysis\n",
    "    frst_deriv_files = glob(join(sub_dir, 'scanner_behav/{0}/Bprime_pmode_set*.txt'.format(sub))) \n",
    "    frst_deriv_files.sort() \n",
    "  \n",
    "   \n",
    "    #grab the behavioral files for all runs\n",
    "    behav_runs = glob(join(sub_dir, 'scanner_behav/{0}/{0}_wmazebl_2015*.txt'.format(sub))) \n",
    "    behav_runs.sort()       \n",
    "    \n",
    "    for i, curr_set in enumerate(sets):\n",
    "        deriv_file = np.loadtxt(frst_deriv_files[i])\n",
    "        learning_curve = np.loadtxt(learning_files[i])      \n",
    "        behav_run1 = pd.read_table(behav_runs[i * 2])\n",
    "        behav_run2 = pd.read_table(behav_runs[i * 2 + 1]) \n",
    "\n",
    "        behav_os = np.empty(320, dtype=object)\n",
    "        behav_os[:160] = behav_run1['StimOnset'].values\n",
    "        behav_os[160:] = behav_run2['StimOnset'].values + (197*2)\n",
    "\n",
    "        behav_resp = np.empty(320, dtype=object)\n",
    "        behav_resp[:160] = behav_run1['Resp'].values\n",
    "        behav_resp[160:] = behav_run2['Resp'].values\n",
    "\n",
    "        behav_type = np.empty(320, dtype=object)\n",
    "        behav_type[:160] = behav_run1['TrialType'].values\n",
    "        behav_type[160:] = behav_run2['TrialType'].values\n",
    "        \n",
    "        trial_shift = behav_type[:-1] \n",
    "        trial_shift = np.insert(trial_shift, 0, -1)\n",
    "        trial_shift2 = behav_type[1:] \n",
    "        trial_shift2 = np.append(trial_shift2, -1)\n",
    "       \n",
    "        \n",
    "        # get all_remaining and nonresponse bullshit out of the way now\n",
    "        all_remaining_with_Bs = np.where((trial_shift2 != 'B') | (behav_type == 'BL'))\n",
    "        all_non_Bs = np.where((behav_type != 'B'))\n",
    "        #print \"All REMAINING #1\", all_remaining_with_Bs\n",
    "        #print \"All REMAINING #2\", all_non_Bs\n",
    "     \n",
    "        \n",
    "        ##GET ALL B's, CREATE LIST OF BAD B TRIAL INDICES\n",
    "        #indices of all Bs in the original dataset without preceeding BL trials\n",
    "        b_indices = np.where((behav_type == 'B'))[0]\n",
    "        #print '1', b_indices\n",
    "        #grabs B trials with preceeding BLs\n",
    "        b_BL_indices = np.where((behav_type == 'B') & (trial_shift == 'BL'))[0]\n",
    "        #print '2', b_BL_indices\n",
    "        #isolate bad Bs for removal in learning curve/derivative\n",
    "        bad_Bs = [] \n",
    "        bad_Bs.extend(b_BL_indices)\n",
    "        for curr_B in b_indices:\n",
    "            if not curr_B in bad_Bs:\n",
    "                #identify in B trials which are non-response\n",
    "                if behav_resp[curr_B] == 'NR': \n",
    "                    bad_Bs.append(curr_B)\n",
    "                #indices if B trial comes first (observed on 001 run 6)    \n",
    "                if curr_B in [0, 157, 158, 159, 160, 317, 318, 319]: \n",
    "                    bad_Bs.append(curr_B)\n",
    "        bad_Bs.sort()\n",
    "        #print len(bad_Bs)\n",
    "        #print '3', bad_Bs\n",
    "        #get the indices for the bad Bs within the group of Bs\n",
    "        bad_B_ind = [j for j, f in enumerate(b_indices) if f in bad_Bs]\n",
    "        #print '4', bad_B_ind\n",
    "        bad_B_ind.sort()\n",
    "        #reverse order of Bs to be removed\n",
    "        bad_B_ind = bad_B_ind[::-1] ##BAD B INDICES\n",
    "        #print len(bad_Bs)\n",
    "        bad_B_onsets = behav_os[bad_Bs]\n",
    "        \n",
    "        #add fixed before bad Bs to all_remaining\n",
    "        fixed_bads = [x - 1 for x in bad_Bs]\n",
    "        fixed_bad_onsets = behav_os[fixed_bads]\n",
    "\n",
    "        #add bad Bs back to all_remaining for Conditional Analysis\n",
    "        all_non_Bs_onsets = behav_os[all_non_Bs[0]]\n",
    "        all_remaining_onsets = np.hstack((all_non_Bs_onsets, bad_B_onsets))\n",
    "        all_remaining_onsets.sort()\n",
    "                \n",
    "        all_remaining_Bs_mtrx = np.vstack((all_remaining_Bs_onsets,\n",
    "                                           np.ones(len(all_remaining_Bs_onsets)) * 3.0,\n",
    "                                           np.ones(len(all_remaining_Bs_onsets)))).T\n",
    "        all_remaining_mtrx = np.vstack((all_remaining_onsets,\n",
    "                                        np.ones(len(all_remaining_onsets)) * 3.0,\n",
    "                                        np.ones(len(all_remaining_onsets)))).T\n",
    "        \n",
    "                      \n",
    "        ###LEARNING CURVE FILES\n",
    "        #create a temp learning_curve\n",
    "        temp = list(learning_curve[:-1])\n",
    "        #pop out the bad Bs\n",
    "        for curr_bad_B in bad_B_ind:\n",
    "            temp.pop(curr_bad_B)\n",
    "        #learning curve values with bad Bs removed   \n",
    "        new_learning_val = np.array(temp) ##learning curve values for good_Bs indices\n",
    "        #print 'learning curve', len(new_learning_val), new_learning_val\n",
    "        #create index array length of learning array\n",
    "        #new_learning_ind = np.arange(0, len(new_learning_val))\n",
    "        #print 'new index list', new_learning_ind \n",
    "        sort_learning_indices = np.argsort(new_learning_val) ##provides sorted indices from low to high performance\n",
    "        #print 'sorted index list', len(sort_learning_indices), sort_learning_indices\n",
    "\n",
    "        \n",
    "        ##CREATE NEW LIST OF B INDICES WITH BAD B INDICES REMOVED\n",
    "        #remove the bad Bs from the B-list\n",
    "        temp1 = list(b_indices)\n",
    "        for curr_bad_B in bad_B_ind:\n",
    "            temp1.pop(curr_bad_B)\n",
    "        good_Bs = np.array(temp1) ##index (original behavior file) for good B trials \n",
    "        #print 'Good B Indices', len(good_Bs), good_Bs\n",
    "        learning_ind = good_Bs[sort_learning_indices]\n",
    "        #print 'Learning Indices', len(learning_ind), learning_ind    \n",
    "        \n",
    "        #divide corrected learning curve into 4 bins\n",
    "        l_bin_1 = learning_ind[:len(learning_ind)/3]\n",
    "        l_bin_1.sort()\n",
    "        l_bin_2 = learning_ind[len(learning_ind)/3:len(learning_ind)*2/3]\n",
    "        l_bin_2.sort()\n",
    "        l_bin_3 = learning_ind[len(learning_ind)*2/3:]\n",
    "        l_bin_3.sort()\n",
    "        \n",
    "          \n",
    "        ####COND####\n",
    "                \n",
    "        #get onsets for trials in each bin\n",
    "        cl_bin_1_onsets = behav_os[l_bin_1]\n",
    "        cl_bin_2_onsets = behav_os[l_bin_2]\n",
    "        cl_bin_3_onsets = behav_os[l_bin_3]\n",
    "\n",
    "        \n",
    "        #use v-stack to vertically stack onsets, duration, and amplitude\n",
    "        learn_mtrx = np.vstack((behav_os,\n",
    "                                np.ones(len(behav_os)) * 3.0,\n",
    "                                np.ones(len(behav_os)))).T   \n",
    "        cl_bin_1_mtrx = np.vstack((cl_bin_1_onsets,\n",
    "                                  np.ones(len(cl_bin_1_onsets)) * 3.0,\n",
    "                                  np.ones(len(cl_bin_1_onsets)))).T\n",
    "        cl_bin_2_mtrx = np.vstack((cl_bin_2_onsets,\n",
    "                                  np.ones(len(cl_bin_2_onsets)) * 3.0,\n",
    "                                  np.ones(len(cl_bin_2_onsets)))).T        \n",
    "        cl_bin_3_mtrx = np.vstack((cl_bin_3_onsets,\n",
    "                                  np.ones(len(cl_bin_3_onsets)) * 3.0,\n",
    "                                  np.ones(len(cl_bin_3_onsets)))).T  \n",
    "        \n",
    "        ####FIXED####\n",
    "        \n",
    "        #subtract 1 from each value to get index of the fixed preceding\n",
    "        fixed_bin1_ind = [x - 1 for x in l_bin_1]\n",
    "        fixed_bin2_ind = [x - 1 for x in l_bin_2]\n",
    "        fixed_bin3_ind = [x - 1 for x in l_bin_3]\n",
    "        #print fixed_bin3_ind\n",
    "        #print 'Fixed Before Learning', fixed_learning_ind\n",
    "        \n",
    "        #get onsets for trials in each bin\n",
    "        fl_bin_1_onsets = behav_os[fixed_bin1_ind]\n",
    "        fl_bin_2_onsets = behav_os[fixed_bin2_ind]\n",
    "        fl_bin_3_onsets = behav_os[fixed_bin3_ind]\n",
    "        \n",
    "        #use v-stack to vertically stack onsets, duration, and amplitude\n",
    "        flearn_mtrx = np.vstack((behav_os,\n",
    "                                 np.ones(len(behav_os)) * 3.0,\n",
    "                                 np.ones(len(behav_os)))).T   \n",
    "        fl_bin_1_mtrx = np.vstack((fl_bin_1_onsets,\n",
    "                                   np.ones(len(fl_bin_1_onsets)) * 3.0,\n",
    "                                   np.ones(len(fl_bin_1_onsets)))).T\n",
    "        fl_bin_2_mtrx = np.vstack((fl_bin_2_onsets,\n",
    "                                   np.ones(len(fl_bin_2_onsets)) * 3.0,\n",
    "                                   np.ones(len(fl_bin_2_onsets)))).T        \n",
    "        fl_bin_3_mtrx = np.vstack((fl_bin_3_onsets,\n",
    "                                   np.ones(len(fl_bin_3_onsets)) * 3.0,\n",
    "                                   np.ones(len(fl_bin_3_onsets)))).T              \n",
    "\n",
    "\n",
    "        if not os.path.exists(join(save_dir, 'MRthesis/', 'model_curve/', 'EVs/')):\n",
    "            os.makedirs(join(save_dir, 'MRthesis/', 'model_curve/', 'EVs/')) \n",
    "\n",
    "        ###COND LEARNING     \n",
    "        np.savetxt(save_dir + '/MRthesis/' + 'model_curve/' + 'EVs/' + \n",
    "                   '{0}_all_learn.txt'.format(curr_set), learn_mtrx, delimiter='\\t', fmt='%.4f')\n",
    "        # Learning Bin 1\n",
    "        np.savetxt(save_dir + '/MRthesis/' + 'model_curve/' + 'EVs/' + \n",
    "                   '{0}_cond_bin_1.txt'.format(curr_set), cl_bin_1_mtrx, delimiter='\\t', fmt='%.4f')\n",
    "        # Learning Bin 2\n",
    "        np.savetxt(save_dir + '/MRthesis/' + 'model_curve/' + 'EVs/' + \n",
    "                   '{0}_cond_bin_2.txt'.format(curr_set), cl_bin_2_mtrx, delimiter='\\t', fmt='%.4f')\n",
    "        # Learning Bin 3\n",
    "        np.savetxt(save_dir + '/MRthesis/' + 'model_curve/' + 'EVs/' + \n",
    "                   '{0}_cond_bin_3.txt'.format(curr_set), cl_bin_3_mtrx, delimiter='\\t', fmt='%.4f')\n",
    "        \n",
    "        ###FIXED LEARNING     \n",
    "        # Learning Bin 1\n",
    "        np.savetxt(save_dir + '/MRthesis/' + 'model_curve/' + 'EVs/' + \n",
    "                   '{0}_fixed_bin_1.txt'.format(curr_set), fl_bin_1_mtrx, delimiter='\\t', fmt='%.4f')\n",
    "        # Learning Bin 2\n",
    "        np.savetxt(save_dir + '/MRthesis/' + 'model_curve/' + 'EVs/' + \n",
    "                   '{0}_fixed_bin_2.txt'.format(curr_set), fl_bin_2_mtrx, delimiter='\\t', fmt='%.4f')\n",
    "        # Learning Bin 3\n",
    "        np.savetxt(save_dir + '/MRthesis/' + 'model_curve/' + 'EVs/' + \n",
    "                   '{0}_fixed_bin_3.txt'.format(curr_set), fl_bin_3_mtrx, delimiter='\\t', fmt='%.4f')  \n",
    "\n",
    "        # All Remaining With Bs\n",
    "        np.savetxt(save_dir + '/MRthesis/' + 'model_curve/' + 'EVs/' + \n",
    "                   '{0}_all_remaining_FB4C.txt'.format(curr_set), \n",
    "                   all_remaining_Bs_mtrx, delimiter='\\t', fmt='%.4f')\n",
    "        # All Remaining Without Bs\n",
    "        np.savetxt(save_dir + '/MRthesis/' + 'model_curve/' + 'EVs/' + \n",
    "                   '{0}_all_remaining_COND.txt'.format(curr_set), \n",
    "                   all_remaining_mtrx, delimiter='\\t', fmt='%.4f')\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#subs = ['WMAZE_001']\n",
    "\n",
    "subs = ['WMAZE_001', 'WMAZE_002', 'WMAZE_004', 'WMAZE_005', 'WMAZE_006',\n",
    "        'WMAZE_007', 'WMAZE_008', 'WMAZE_009', 'WMAZE_010', 'WMAZE_012',\n",
    "        'WMAZE_017', 'WMAZE_018', 'WMAZE_019', 'WMAZE_020', 'WMAZE_021',\n",
    "        'WMAZE_022', 'WMAZE_023', 'WMAZE_024', 'WMAZE_026', 'WMAZE_027']\n",
    "\n",
    "sets = ['set1', 'set2', 'set3']\n",
    "\n",
    "for sub in subs:\n",
    "    sub_dir = '/home/data/madlab/data/mri/wmaze/'\n",
    "    save_dir = '/home/data/madlab/data/mri/wmaze/scanner_behav/{0}/'.format(sub)\n",
    "    \n",
    "    #grab the derivative files created from the set-based learning analysis\n",
    "    frst_deriv_files = glob(join(sub_dir, 'scanner_behav/{0}/Bprime_pmode_set*.txt'.format(sub))) \n",
    "    frst_deriv_files.sort() \n",
    "    \n",
    "    #grab the learning curve file for B trials\n",
    "    learning_files = glob(join(sub_dir, 'scanner_behav/{0}/B_pmode_set*.txt'.format(sub))) \n",
    "    learning_files.sort()\n",
    "   \n",
    "    #grab the behavioral files for all runs\n",
    "    behav_runs = glob(join(sub_dir, 'scanner_behav/{0}/{0}_wmazebl_2015*.txt'.format(sub))) \n",
    "    behav_runs.sort()       \n",
    "    \n",
    "    for i, curr_set in enumerate(sets):\n",
    "        deriv_file = np.loadtxt(frst_deriv_files[i])\n",
    "        learning_curve = np.loadtxt(learning_files[i])      \n",
    "        behav_run1 = pd.read_table(behav_runs[i * 2])\n",
    "        behav_run2 = pd.read_table(behav_runs[i * 2 + 1]) \n",
    "\n",
    "        behav_os = np.empty(320, dtype=object)\n",
    "        behav_os[:160] = behav_run1['StimOnset'].values\n",
    "        behav_os[160:] = behav_run2['StimOnset'].values + (197*2)\n",
    "\n",
    "        behav_resp = np.empty(320, dtype=object)\n",
    "        behav_resp[:160] = behav_run1['Resp'].values\n",
    "        behav_resp[160:] = behav_run2['Resp'].values\n",
    "\n",
    "        behav_type = np.empty(320, dtype=object)\n",
    "        behav_type[:160] = behav_run1['TrialType'].values\n",
    "        behav_type[160:] = behav_run2['TrialType'].values\n",
    "        \n",
    "        trial_shift = behav_type[:-1] \n",
    "        trial_shift = np.insert(trial_shift, 0, -1)\n",
    "        trial_shift2 = behav_type[1:] \n",
    "        trial_shift2 = np.append(trial_shift2, -1)\n",
    "       \n",
    "        \n",
    "        # get all_remaining and nonresponse bullshit out of the way now\n",
    "        all_remaining_with_Bs = np.where((trial_shift2 != 'B') | (behav_type == 'BL'))\n",
    "        all_non_Bs = np.where((behav_type != 'B'))\n",
    "        #print \"All REMAINING #1\", all_remaining_with_Bs\n",
    "        #print \"All REMAINING #2\", all_non_Bs\n",
    "     \n",
    "        \n",
    "        ##GET ALL B's, CREATE LIST OF BAD B TRIAL INDICES\n",
    "        #indices of all Bs in the original dataset without preceeding BL trials\n",
    "        b_indices = np.where((behav_type == 'B'))[0]\n",
    "        #print '1', b_indices\n",
    "        #grabs B trials with preceeding BLs\n",
    "        b_BL_indices = np.where((behav_type == 'B') & (trial_shift == 'BL'))[0]\n",
    "        #print '2', b_BL_indices\n",
    "        #isolate bad Bs for removal in learning curve/derivative\n",
    "        bad_Bs = [] \n",
    "        bad_Bs.extend(b_BL_indices)\n",
    "        for curr_B in b_indices:\n",
    "            if not curr_B in bad_Bs:\n",
    "                #identify in B trials which are non-response\n",
    "                if behav_resp[curr_B] == 'NR': \n",
    "                    bad_Bs.append(curr_B)\n",
    "                #indices if B trial comes first (observed on 001 run 6)    \n",
    "                if curr_B in [0, 157, 158, 159, 160, 317, 318, 319]: \n",
    "                    bad_Bs.append(curr_B)\n",
    "        bad_Bs.sort()\n",
    "        #print len(bad_Bs)\n",
    "        #print '3', bad_Bs\n",
    "        #get the indices for the bad Bs within the group of Bs\n",
    "        bad_B_ind = [j for j, f in enumerate(b_indices) if f in bad_Bs]\n",
    "        #print '4', bad_B_ind\n",
    "        bad_B_ind.sort()\n",
    "        #reverse order of Bs to be removed\n",
    "        bad_B_ind = bad_B_ind[::-1] ##BAD B INDICES\n",
    "        #print len(bad_Bs)\n",
    "        bad_B_onsets = behav_os[bad_Bs]\n",
    "        \n",
    "        #add fixed before bad Bs to all_remaining\n",
    "        fixed_bads = [x - 1 for x in bad_Bs]\n",
    "        fixed_bad_onsets = behav_os[fixed_bads]\n",
    "\n",
    "        #add bad Bs back to all_remaining for Conditional Analysis\n",
    "        all_non_Bs_onsets = behav_os[all_non_Bs[0]]\n",
    "        all_remaining_onsets = np.hstack((all_non_Bs_onsets, bad_B_onsets))\n",
    "        all_remaining_onsets.sort()\n",
    "                \n",
    "        all_remaining_Bs_mtrx = np.vstack((all_remaining_with_Bs_onsets,\n",
    "                                           np.ones(len(all_remaining_with_Bs_onsets)) * 3.0,\n",
    "                                           np.ones(len(all_remaining_with_Bs_onsets)))).T\n",
    "        all_remaining_mtrx = np.vstack((all_remaining_onsets,\n",
    "                                        np.ones(len(all_remaining_onsets)) * 3.0,\n",
    "                                        np.ones(len(all_remaining_onsets)))).T\n",
    "        \n",
    "                      \n",
    "        ###LEARNING CURVE FILES\n",
    "        #create a temp learning_curve\n",
    "        temp = list(learning_curve[:-1])\n",
    "        #pop out the bad Bs\n",
    "        for curr_bad_B in bad_B_ind:\n",
    "            temp.pop(curr_bad_B)\n",
    "        #learning curve values with bad Bs removed   \n",
    "        new_learning_val = np.array(temp) ##learning curve values for good_Bs indices\n",
    "        #print 'learning curve', len(new_learning_val), new_learning_val\n",
    "        #create index array length of learning array\n",
    "        #new_learning_ind = np.arange(0, len(new_learning_val))\n",
    "        #print 'new index list', new_learning_ind \n",
    "        bin1_indices = np.where((new_learning_val >= 0.40) & (new_learning_val <= 0.59))[0]\n",
    "        bin2_indices = np.where((new_learning_val >= 0.60) & (new_learning_val <= 0.79))[0]\n",
    "        bin3_indices = np.where((new_learning_val >= 0.80) & (new_learning_val <= 0.99))[0]\n",
    "        print sub, curr_set\n",
    "        print \"#1\", len(bin1_indices)\n",
    "        print \"#2\", len(bin2_indices)\n",
    "        print \"#3\", len(bin3_indices)\n",
    "        print \"\"\n",
    "        \n",
    "        ##CREATE NEW LIST OF B INDICES WITH BAD B INDICES REMOVED\n",
    "        #remove the bad Bs from the B-list\n",
    "        temp1 = list(b_indices)\n",
    "        for curr_bad_B in bad_B_ind:\n",
    "            temp1.pop(curr_bad_B)\n",
    "        good_Bs = np.array(temp1) ##index (original behavior file) for good B trials \n",
    "        #print 'Good B Indices', len(good_Bs), good_Bs\n",
    "        bin1_ind = good_Bs[bin1_indices]\n",
    "        bin2_ind = good_Bs[bin2_indices]\n",
    "        bin3_ind = good_Bs[bin3_indices]\n",
    "        #print bin1_ind\n",
    "        #print bin2_ind\n",
    "        #print bin3_ind\n",
    "        #print \"\"\n",
    "  \n",
    "        ####COND####\n",
    "    \n",
    "        #get onsets for trials in each bin\n",
    "        cl_bin_1_onsets = behav_os[bin1_ind]\n",
    "        cl_bin_2_onsets = behav_os[bin2_ind]\n",
    "        cl_bin_3_onsets = behav_os[bin3_ind]\n",
    "\n",
    "        \n",
    "        #use v-stack to vertically stack onsets, duration, and amplitude\n",
    "        learn_mtrx = np.vstack((behav_os,\n",
    "                                np.ones(len(behav_os)) * 3.0,\n",
    "                                np.ones(len(behav_os)))).T   \n",
    "        cl_bin_1_mtrx = np.vstack((cl_bin_1_onsets,\n",
    "                                  np.ones(len(cl_bin_1_onsets)) * 3.0,\n",
    "                                  np.ones(len(cl_bin_1_onsets)))).T\n",
    "        cl_bin_2_mtrx = np.vstack((cl_bin_2_onsets,\n",
    "                                  np.ones(len(cl_bin_2_onsets)) * 3.0,\n",
    "                                  np.ones(len(cl_bin_2_onsets)))).T        \n",
    "        cl_bin_3_mtrx = np.vstack((cl_bin_3_onsets,\n",
    "                                  np.ones(len(cl_bin_3_onsets)) * 3.0,\n",
    "                                  np.ones(len(cl_bin_3_onsets)))).T  \n",
    "        \n",
    "        ####FIXED####\n",
    "        \n",
    "        #subtract 1 from each value to get index of the fixed preceding\n",
    "        fixed_bin1_ind = [x - 1 for x in bin1_ind]\n",
    "        fixed_bin2_ind = [x - 1 for x in bin2_ind]\n",
    "        fixed_bin3_ind = [x - 1 for x in bin3_ind]\n",
    "        #print fixed_bin3_ind\n",
    "        #print 'Fixed Before Learning', fixed_learning_ind\n",
    "        \n",
    "        #get onsets for trials in each bin\n",
    "        fl_bin_1_onsets = behav_os[bin1_ind]\n",
    "        fl_bin_2_onsets = behav_os[bin2_ind]\n",
    "        fl_bin_3_onsets = behav_os[bin3_ind]\n",
    "        \n",
    "        #use v-stack to vertically stack onsets, duration, and amplitude\n",
    "        flearn_mtrx = np.vstack((behav_os,\n",
    "                                 np.ones(len(behav_os)) * 3.0,\n",
    "                                 np.ones(len(behav_os)))).T   \n",
    "        fl_bin_1_mtrx = np.vstack((fl_bin_1_onsets,\n",
    "                                   np.ones(len(fl_bin_1_onsets)) * 3.0,\n",
    "                                   np.ones(len(fl_bin_1_onsets)))).T\n",
    "        fl_bin_2_mtrx = np.vstack((fl_bin_2_onsets,\n",
    "                                   np.ones(len(fl_bin_2_onsets)) * 3.0,\n",
    "                                   np.ones(len(fl_bin_2_onsets)))).T        \n",
    "        fl_bin_3_mtrx = np.vstack((fl_bin_3_onsets,\n",
    "                                   np.ones(len(fl_bin_3_onsets)) * 3.0,\n",
    "                                   np.ones(len(fl_bin_3_onsets)))).T              \n",
    "\n",
    "\n",
    "        if not os.path.exists(join(save_dir, 'MRthesis/', 'model_learn1_curve/', 'EVs/')):\n",
    "            os.makedirs(join(save_dir, 'MRthesis/', 'model_learn1_curve/', 'EVs/')) \n",
    "\n",
    "        ###COND LEARNING     \n",
    "        np.savetxt(save_dir + '/MRthesis/' + 'model_learn1_curve/' + 'EVs/' + \n",
    "                   '{0}_all_learn.txt'.format(curr_set), learn_mtrx, delimiter='\\t', fmt='%.4f')\n",
    "        # Learning Bin 1\n",
    "        np.savetxt(save_dir + '/MRthesis/' + 'model_learn1_curve/' + 'EVs/' + \n",
    "                   '{0}_cond_bin_1.txt'.format(curr_set), cl_bin_1_mtrx, delimiter='\\t', fmt='%.4f')\n",
    "        # Learning Bin 2\n",
    "        np.savetxt(save_dir + '/MRthesis/' + 'model_learn1_curve/' + 'EVs/' + \n",
    "                   '{0}_cond_bin_2.txt'.format(curr_set), cl_bin_2_mtrx, delimiter='\\t', fmt='%.4f')\n",
    "        # Learning Bin 3\n",
    "        np.savetxt(save_dir + '/MRthesis/' + 'model_learn1_curve/' + 'EVs/' + \n",
    "                   '{0}_cond_bin_3.txt'.format(curr_set), cl_bin_3_mtrx, delimiter='\\t', fmt='%.4f')\n",
    "        \n",
    "        ###FIXED LEARNING     \n",
    "        # Learning Bin 1\n",
    "        np.savetxt(save_dir + '/MRthesis/' + 'model_learn1_curve/' + 'EVs/' + \n",
    "                   '{0}_fixed_bin_1.txt'.format(curr_set), fl_bin_1_mtrx, delimiter='\\t', fmt='%.4f')\n",
    "        # Learning Bin 2\n",
    "        np.savetxt(save_dir + '/MRthesis/' + 'model_learn1_curve/' + 'EVs/' + \n",
    "                   '{0}_fixed_bin_2.txt'.format(curr_set), fl_bin_2_mtrx, delimiter='\\t', fmt='%.4f')\n",
    "        # Learning Bin 3\n",
    "        np.savetxt(save_dir + '/MRthesis/' + 'model_learn1_curve/' + 'EVs/' + \n",
    "                   '{0}_fixed_bin_3.txt'.format(curr_set), fl_bin_3_mtrx, delimiter='\\t', fmt='%.4f')  \n",
    "\n",
    "        # All Remaining With Bs\n",
    "        np.savetxt(save_dir + '/MRthesis/' + 'model_learn1_curve/' + 'EVs/' + \n",
    "                   '{0}_all_remaining_FB4C.txt'.format(curr_set), \n",
    "                   all_remaining_Bs_mtrx, delimiter='\\t', fmt='%.4f')\n",
    "        # All Remaining Without Bs\n",
    "        np.savetxt(save_dir + '/MRthesis/' + 'model_learn1_curve/' + 'EVs/' + \n",
    "                   '{0}_all_remaining_COND.txt'.format(curr_set), \n",
    "                   all_remaining_mtrx, delimiter='\\t', fmt='%.4f')\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#subs = ['WMAZE_001']\n",
    "\n",
    "subs = ['WMAZE_001', 'WMAZE_002', 'WMAZE_004', 'WMAZE_005', 'WMAZE_006',\n",
    "        'WMAZE_007', 'WMAZE_008', 'WMAZE_009', 'WMAZE_010', 'WMAZE_012',\n",
    "        'WMAZE_017', 'WMAZE_018', 'WMAZE_019', 'WMAZE_020', 'WMAZE_021',\n",
    "        'WMAZE_022', 'WMAZE_023', 'WMAZE_024', 'WMAZE_026', 'WMAZE_027']\n",
    "\n",
    "sets = ['set1', 'set2', 'set3']\n",
    "\n",
    "\n",
    "for sub in subs:\n",
    "    sub_dir = '/home/data/madlab/data/mri/wmaze/'\n",
    "    save_dir = '/home/data/madlab/data/mri/wmaze/scanner_behav/{0}/'.format(sub)\n",
    "    \n",
    "    #grab the derivative files created from the set-based learning analysis\n",
    "    frst_deriv_files = glob(join(sub_dir, 'scanner_behav/{0}/Bprime_pmode_set*.txt'.format(sub))) \n",
    "    frst_deriv_files.sort() \n",
    "    \n",
    "    #grab the learning curve file for B trials\n",
    "    learning_files = glob(join(sub_dir, 'scanner_behav/{0}/B_pmode_set*.txt'.format(sub))) \n",
    "    learning_files.sort()\n",
    "   \n",
    "    #grab the behavioral files for all runs\n",
    "    behav_runs = glob(join(sub_dir, 'scanner_behav/{0}/{0}_wmazebl_2015*.txt'.format(sub))) \n",
    "    behav_runs.sort()       \n",
    "    \n",
    "    for i, curr_set in enumerate(sets):\n",
    "        deriv_file = np.loadtxt(frst_deriv_files[i])\n",
    "        learning_curve = np.loadtxt(learning_files[i])      \n",
    "        behav_run1 = pd.read_table(behav_runs[i * 2])\n",
    "        behav_run2 = pd.read_table(behav_runs[i * 2 + 1]) \n",
    "\n",
    "        behav_os = np.empty(320, dtype=object)\n",
    "        behav_os[:160] = behav_run1['StimOnset'].values\n",
    "        behav_os[160:] = behav_run2['StimOnset'].values + (197*2)\n",
    "\n",
    "        behav_resp = np.empty(320, dtype=object)\n",
    "        behav_resp[:160] = behav_run1['Resp'].values\n",
    "        behav_resp[160:] = behav_run2['Resp'].values\n",
    "\n",
    "        behav_type = np.empty(320, dtype=object)\n",
    "        behav_type[:160] = behav_run1['TrialType'].values\n",
    "        behav_type[160:] = behav_run2['TrialType'].values\n",
    "        \n",
    "        trial_shift = behav_type[:-1] \n",
    "        trial_shift = np.insert(trial_shift, 0, -1)  \n",
    "        \n",
    "        # get all_remaining and nonresponse bullshit out of the way now\n",
    "        all_remaining = np.where((trial_shift != 'B') & (behav_resp != 'NR') | (behav_type == 'BL'))\n",
    "        nonresponse = np.where((behav_resp == 'NR'))\n",
    "        \n",
    "        all_remaining_onsets = behav_os[all_remaining[0]]\n",
    "        nonresponse_onsets = behav_os[nonresponse[0]] \n",
    "        \n",
    "        all_remaining_mtrx = np.vstack((all_remaining_onsets,\n",
    "                                        np.ones(len(all_remaining_onsets)) * 3.0,\n",
    "                                        np.ones(len(all_remaining_onsets)))).T\n",
    "        nonresponse_mtrx = np.vstack((nonresponse_onsets,\n",
    "                                      np.ones(len(nonresponse_onsets)) * 3.0,\n",
    "                                      np.ones(len(nonresponse_onsets)))).T\n",
    "            \n",
    "        ##GET ALL B's, CREATE LIST OF BAD B TRIAL INDICES\n",
    "        #indices of all Bs in the original dataset without preceeding BL trials\n",
    "        b_indices = np.where((behav_type == 'B'))[0]\n",
    "        #print '1', b_indices\n",
    "        #grabs B trials with preceeding BLs\n",
    "        b_BL_indices = np.where((behav_type == 'B') & (trial_shift == 'BL'))[0]\n",
    "        #print '2', b_BL_indices\n",
    "        #isolate bad Bs for removal in learning curve/derivative\n",
    "        bad_Bs = [] \n",
    "        bad_Bs.extend(b_BL_indices)\n",
    "        for curr_B in b_indices:\n",
    "            if not curr_B in bad_Bs:\n",
    "                #identify in B trials which are non-response\n",
    "                if behav_resp[curr_B] == 'NR': \n",
    "                    bad_Bs.append(curr_B)\n",
    "                #indices if B trial comes first (observed on 001 run 6)    \n",
    "                if curr_B in [0, 157, 158, 159, 160, 317, 318, 319]: \n",
    "                    bad_Bs.append(curr_B)\n",
    "        bad_Bs.sort()\n",
    "        #print '3', bad_Bs\n",
    "        #get the indices for the bad Bs within the group of Bs\n",
    "        bad_B_ind = [j for j, f in enumerate(b_indices) if f in bad_Bs]\n",
    "        #print '4', bad_B_ind\n",
    "        bad_B_ind.sort()\n",
    "        #reverse order of Bs to be removed\n",
    "        bad_B_ind = bad_B_ind[::-1] ##BAD B INDICES\n",
    "        \n",
    "                      \n",
    "        ###LEARNING CURVE FILES\n",
    "        #create a temp learning_curve\n",
    "        temp = list(learning_curve[:-1])\n",
    "        #pop out the bad Bs\n",
    "        for curr_bad_B in bad_B_ind:\n",
    "            temp.pop(curr_bad_B)\n",
    "        #learning curve values with bad Bs removed   \n",
    "        new_learning_val = np.array(temp) ##learning curve values for good_Bs indices\n",
    "        #print 'learning curve', len(new_learning_val), new_learning_val\n",
    "        #create index array length of learning array\n",
    "        #new_learning_ind = np.arange(0, len(new_learning_val))\n",
    "        #print 'new index list', new_learning_ind \n",
    "        sort_learning_indices = np.argsort(new_learning_val) ##provides sorted indices from low to high performance\n",
    "        #print 'sorted index list', len(sort_learning_indices), sort_learning_indices\n",
    "\n",
    "        \n",
    "        ##CREATE NEW LIST OF B INDICES WITH BAD B INDICES REMOVED\n",
    "        #remove the bad Bs from the B-list\n",
    "        temp1 = list(b_indices)\n",
    "        for curr_bad_B in bad_B_ind:\n",
    "            temp1.pop(curr_bad_B)\n",
    "        good_Bs = np.array(temp1) ##index (original behavior file) for good B trials \n",
    "        #print 'Good B Indices', len(good_Bs), good_Bs\n",
    "        learning_ind = good_Bs[sort_learning_indices]\n",
    "        #print 'Learning Indices', len(learning_ind), learning_ind    \n",
    "        #subtract 1 from each value to get index of the fixed preceding\n",
    "        fixed_learning_ind = [x - 1 for x in learning_ind]\n",
    "        #print '2', fixed_learning_ind\n",
    "\n",
    "        \n",
    "        #divide corrected learning curve into 4 bins\n",
    "        l_bin_1 = learning_ind[:len(fixed_learning_ind)/3]\n",
    "        l_bin_1.sort()\n",
    "        #print l_bin_1\n",
    "        l_bin_2 = learning_ind[len(fixed_learning_ind)/3:len(fixed_learning_ind)*2/3]\n",
    "        l_bin_2.sort()\n",
    "        l_bin_3 = learning_ind[len(fixed_learning_ind)*2/3:]\n",
    "        l_bin_3.sort()\n",
    "                \n",
    "        #get onsets for trials in each bin\n",
    "        l_bin_1_onsets = behav_os[l_bin_1]\n",
    "        l_bin_2_onsets = behav_os[l_bin_2]\n",
    "        l_bin_3_onsets = behav_os[l_bin_3]\n",
    "        \n",
    "        \n",
    "        #use v-stack to vertically stack onsets, duration, and amplitude\n",
    "        learn_mtrx = np.vstack((behav_os,\n",
    "                                np.ones(len(behav_os)) * 3.0,\n",
    "                                np.ones(len(behav_os)))).T        \n",
    "        l_bin_1_mtrx = np.vstack((l_bin_1_onsets,\n",
    "                                  np.ones(len(l_bin_1_onsets)) * 3.0,\n",
    "                                  np.ones(len(l_bin_1_onsets)))).T\n",
    "        l_bin_2_mtrx = np.vstack((l_bin_2_onsets,\n",
    "                                  np.ones(len(l_bin_2_onsets)) * 3.0,\n",
    "                                  np.ones(len(l_bin_2_onsets)))).T        \n",
    "        l_bin_3_mtrx = np.vstack((l_bin_3_onsets,\n",
    "                                  np.ones(len(l_bin_3_onsets)) * 3.0,\n",
    "                                  np.ones(len(l_bin_3_onsets)))).T\n",
    "        \n",
    "        \n",
    "        ###DERIV FILES\n",
    "        temp2 = list(deriv_file)\n",
    "        for curr_bad_B in bad_B_ind:\n",
    "            temp2.pop(curr_bad_B)   \n",
    "        new_deriv_val = np.array(temp2[:-1])\n",
    "        new_deriv_ind = np.arange(0, len(new_deriv_val)) \n",
    "        positive_derivs_val = np.array([f for f in new_deriv_val if f > 0])\n",
    "        #positive_derivs_ind = np.array([j for j, f in enumerate(new_deriv_val) if f > 0])\n",
    "        #print len(positive_derivs_ind)\n",
    "        negative_derivs_val = np.array([f for f in new_deriv_val if f <= 0])\n",
    "        #negative_derivs_ind = np.array([j for j, f in enumerate(new_deriv_val) if f <= 0])\n",
    "        sort_pos_deriv_indices = np.argsort(positive_derivs_val)\n",
    "        sort_neg_deriv_indices = np.argsort(negative_derivs_val)\n",
    "        positive_derivs_ind = good_Bs[sort_pos_deriv_indices]\n",
    "        negative_derivs_ind = good_Bs[sort_neg_deriv_indices] \n",
    "        #grab fixed before\n",
    "        fixed_pos_derivs = [x - 1 for x in positive_derivs_ind]\n",
    "        fixed_neg_derivs = [x - 1 for x in negative_derivs_ind]\n",
    "        \n",
    "        \n",
    "        #binned indices\n",
    "        d_bin_0 = fixed_neg_derivs[:]\n",
    "        d_bin_0.sort()\n",
    "        d_bin_1 = fixed_pos_derivs[:len(fixed_pos_derivs)/3]\n",
    "        d_bin_1.sort()\n",
    "        d_bin_2 = fixed_pos_derivs[len(fixed_pos_derivs)/3:len(fixed_pos_derivs)*2/3]\n",
    "        d_bin_2.sort()\n",
    "        d_bin_3 = fixed_pos_derivs[len(fixed_pos_derivs)*2/3:]\n",
    "        d_bin_3.sort()\n",
    "        \n",
    "        d_bin_0_onsets = behav_os[d_bin_0]\n",
    "        d_bin_1_onsets = behav_os[d_bin_1]\n",
    "        d_bin_2_onsets = behav_os[d_bin_2]\n",
    "        d_bin_3_onsets = behav_os[d_bin_3]\n",
    "        \n",
    "        deriv_mtrx = np.vstack((behav_os,\n",
    "                                np.ones(len(behav_os)) * 3.0,\n",
    "                                np.ones(len(behav_os)))).T        \n",
    "        d_bin_0_mtrx = np.vstack((d_bin_0_onsets,\n",
    "                                  np.ones(len(d_bin_0_onsets)) * 3.0,\n",
    "                                  np.ones(len(d_bin_0_onsets)))).T\n",
    "        d_bin_1_mtrx = np.vstack((d_bin_1_onsets,\n",
    "                                  np.ones(len(d_bin_1_onsets)) * 3.0,\n",
    "                                  np.ones(len(d_bin_1_onsets)))).T        \n",
    "        d_bin_2_mtrx = np.vstack((d_bin_2_onsets,\n",
    "                                  np.ones(len(d_bin_2_onsets)) * 3.0,\n",
    "                                  np.ones(len(d_bin_2_onsets)))).T        \n",
    "        d_bin_3_mtrx = np.vstack((d_bin_3_onsets,\n",
    "                                  np.ones(len(d_bin_3_onsets)) * 3.0,\n",
    "                                  np.ones(len(d_bin_3_onsets)))).T\n",
    "\n",
    "        \n",
    "        if not os.path.exists(join(save_dir, 'MRthesis/', 'model_learn_curve/', 'EVs/')):\n",
    "            os.makedirs(join(save_dir, 'MRthesis/', 'model_learn_curve/', 'EVs/')) \n",
    "\n",
    "        ###all learn     \n",
    "        np.savetxt(save_dir + '/MRthesis/' + 'model_learn_curve/' + 'EVs/' + \n",
    "                   '{0}_all_learn.txt'.format(curr_set), learn_mtrx, delimiter='\\t', fmt='%.4f')\n",
    "        # Learning Bin 1\n",
    "        np.savetxt(save_dir + '/MRthesis/' + 'model_learn_curve/' + 'EVs/' + \n",
    "                   '{0}_l_bin_1.txt'.format(curr_set), l_bin_1_mtrx, delimiter='\\t', fmt='%.4f')\n",
    "        # Learning Bin 2\n",
    "        np.savetxt(save_dir + '/MRthesis/' + 'model_learn_curve/' + 'EVs/' + \n",
    "                   '{0}_l_bin_2.txt'.format(curr_set), l_bin_2_mtrx, delimiter='\\t', fmt='%.4f')\n",
    "        # Learning Bin 3\n",
    "        np.savetxt(save_dir + '/MRthesis/' + 'model_learn_curve/' + 'EVs/' + \n",
    "                   '{0}_l_bin_3.txt'.format(curr_set), l_bin_3_mtrx, delimiter='\\t', fmt='%.4f')\n",
    "        # Learning Bin 4\n",
    "        np.savetxt(save_dir + '/MRthesis/' + 'model_learn_curve/' + 'EVs/' + \n",
    "                   '{0}_l_bin_4.txt'.format(curr_set), l_bin_4_mtrx, delimiter='\\t', fmt='%.4f')\n",
    "        \n",
    "        ###all deriv     \n",
    "        np.savetxt(save_dir + '/MRthesis/' + 'model_learn_curve/' + 'EVs/' + \n",
    "                   '{0}_all_deriv.txt'.format(curr_set), deriv_mtrx, delimiter='\\t', fmt='%.4f')\n",
    "        # Deriv Bin 0\n",
    "        np.savetxt(save_dir + '/MRthesis/' + 'model_learn_curve/' + 'EVs/' + \n",
    "                   '{0}_d_bin_0.txt'.format(curr_set), d_bin_0_mtrx, delimiter='\\t', fmt='%.4f')\n",
    "        # Deriv Bin 1\n",
    "        np.savetxt(save_dir + '/MRthesis/' + 'model_learn_curve/' + 'EVs/' + \n",
    "                   '{0}_d_bin_1.txt'.format(curr_set), d_bin_1_mtrx, delimiter='\\t', fmt='%.4f')\n",
    "        # Deriv Bin 2\n",
    "        np.savetxt(save_dir + '/MRthesis/' + 'model_learn_curve/' + 'EVs/' + \n",
    "                   '{0}_d_bin_2.txt'.format(curr_set), d_bin_2_mtrx, delimiter='\\t', fmt='%.4f')\n",
    "        # Deriv Bin 3\n",
    "        np.savetxt(save_dir + '/MRthesis/' + 'model_learn_curve/' + 'EVs/' + \n",
    "                   '{0}_d_bin_3.txt'.format(curr_set), d_bin_3_mtrx, delimiter='\\t', fmt='%.4f')\n",
    "        \n",
    "        # All Remaining\n",
    "        np.savetxt(save_dir + '/MRthesis/' + 'model_learn_curve/' + 'EVs/' + \n",
    "                   '{0}_all_remaining.txt'.format(curr_set), all_remaining_mtrx, delimiter='\\t', fmt='%.4f')\n",
    "        # Nonresponse\n",
    "        np.savetxt(save_dir + '/MRthesis/' + 'model_learn_curve/' + 'EVs/' + \n",
    "                   '{0}_nonresponse.txt'.format(curr_set), nonresponse_mtrx, delimiter='\\t', fmt='%.4f')\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
